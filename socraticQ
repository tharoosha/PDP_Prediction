{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9752919,"sourceType":"datasetVersion","datasetId":5971378},{"sourceId":9753225,"sourceType":"datasetVersion","datasetId":5971596}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-31T12:41:08.102163Z","iopub.execute_input":"2024-10-31T12:41:08.102544Z","iopub.status.idle":"2024-10-31T12:41:08.492713Z","shell.execute_reply.started":"2024-10-31T12:41:08.102499Z","shell.execute_reply":"2024-10-31T12:41:08.491774Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/train-data/good_outputs.json\n/kaggle/input/train-data/preference_data.json\n/kaggle/input/train-data/input_prompts.json\n/kaggle/input/train-data/problem_metadata.json\n/kaggle/input/test-data/good_outputs.json\n/kaggle/input/test-data/input_prompts.json\n/kaggle/input/test-data/problem_metadata.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The trl library is a full stack tool to fine-tune and align transformer language and diffusion models using methods such as Supervised Fine-tuning step (SFT), Reward Modeling (RM) and the Proximal Policy Optimization (PPO) as well as Direct Preference Optimization (DPO).","metadata":{}},{"cell_type":"code","source":"# pip install trl","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:41:08.494720Z","iopub.execute_input":"2024-10-31T12:41:08.495353Z","iopub.status.idle":"2024-10-31T12:41:08.499399Z","shell.execute_reply.started":"2024-10-31T12:41:08.495303Z","shell.execute_reply":"2024-10-31T12:41:08.498446Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:41:08.500631Z","iopub.execute_input":"2024-10-31T12:41:08.500990Z","iopub.status.idle":"2024-10-31T12:41:08.509702Z","shell.execute_reply.started":"2024-10-31T12:41:08.500947Z","shell.execute_reply":"2024-10-31T12:41:08.508885Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# pip install -U bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:41:08.511605Z","iopub.execute_input":"2024-10-31T12:41:08.511937Z","iopub.status.idle":"2024-10-31T12:41:08.520375Z","shell.execute_reply.started":"2024-10-31T12:41:08.511905Z","shell.execute_reply":"2024-10-31T12:41:08.519560Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:03:38.233761Z","iopub.execute_input":"2024-10-31T15:03:38.234525Z","iopub.status.idle":"2024-10-31T15:03:38.529363Z","shell.execute_reply.started":"2024-10-31T15:03:38.234478Z","shell.execute_reply":"2024-10-31T15:03:38.528408Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama-3.1-8B-bnb-4bit on Math Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:41:09.300097Z","iopub.execute_input":"2024-10-31T12:41:09.300435Z","iopub.status.idle":"2024-10-31T12:41:16.514689Z","shell.execute_reply.started":"2024-10-31T12:41:09.300397Z","shell.execute_reply":"2024-10-31T12:41:16.513729Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtharooshavihidun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111294735555551, max=1.0)â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b9334a541214e8aafefe8aadb568357"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241031_124112-f2sfivpe</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tharooshavihidun/Fine-tune%20Llama-3.1-8B-bnb-4bit%20on%20Math%20Dataset/runs/f2sfivpe' target=\"_blank\">somber-howl-10</a></strong> to <a href='https://wandb.ai/tharooshavihidun/Fine-tune%20Llama-3.1-8B-bnb-4bit%20on%20Math%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tharooshavihidun/Fine-tune%20Llama-3.1-8B-bnb-4bit%20on%20Math%20Dataset' target=\"_blank\">https://wandb.ai/tharooshavihidun/Fine-tune%20Llama-3.1-8B-bnb-4bit%20on%20Math%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tharooshavihidun/Fine-tune%20Llama-3.1-8B-bnb-4bit%20on%20Math%20Dataset/runs/f2sfivpe' target=\"_blank\">https://wandb.ai/tharooshavihidun/Fine-tune%20Llama-3.1-8B-bnb-4bit%20on%20Math%20Dataset/runs/f2sfivpe</a>"},"metadata":{}}]},{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:41:16.516010Z","iopub.execute_input":"2024-10-31T12:41:16.516330Z","iopub.status.idle":"2024-10-31T12:44:41.852236Z","shell.execute_reply.started":"2024-10-31T12:41:16.516297Z","shell.execute_reply":"2024-10-31T12:44:41.851282Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n    \"unsloth/llama-2-7b-bnb-4bit\",\n    \"unsloth/llama-2-13b-bnb-4bit\",\n    \"unsloth/codellama-34b-bnb-4bit\",\n    \"unsloth/tinyllama-bnb-4bit\",\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/mistral-7b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:44:41.853951Z","iopub.execute_input":"2024-10-31T12:44:41.854759Z","iopub.status.idle":"2024-10-31T12:46:51.989804Z","shell.execute_reply.started":"2024-10-31T12:44:41.854709Z","shell.execute_reply":"2024-10-31T12:46:51.988871Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n==((====))==  Unsloth 2024.10.7: Fast Mistral patching. Transformers = 4.44.2.\n   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.888 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.5.1+cu121. CUDA = 6.0. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58baff44f8fa4a4b94db2f51e845b923"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/155 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03dee35527db455797a517625955e6cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdc40359aa404fe39e0ca847d305e2ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d8855daea584adeb2702e91186bf070"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0ee2e2312234861bf6eee2db631b72d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60bd23cdddce4f36919d00fac84bbf44"}},"metadata":{}},{"name":"stderr","text":"Unsloth: We fixed a gradient accumulation bug, but it seems like you don't have the latest transformers version!\nPlease update transformers, TRL and unsloth via:\n`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n","output_type":"stream"}]},{"cell_type":"code","source":"# pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:51.991058Z","iopub.execute_input":"2024-10-31T12:46:51.991719Z","iopub.status.idle":"2024-10-31T12:46:51.996810Z","shell.execute_reply.started":"2024-10-31T12:46:51.991681Z","shell.execute_reply":"2024-10-31T12:46:51.995835Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# !pip install --no-deps xformers trl peft accelerate bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:52.000453Z","iopub.execute_input":"2024-10-31T12:46:52.000808Z","iopub.status.idle":"2024-10-31T12:46:52.009710Z","shell.execute_reply.started":"2024-10-31T12:46:52.000777Z","shell.execute_reply":"2024-10-31T12:46:52.008835Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# from typing import Optional\n# # import argparse\n# import os\n# import sys\n# import json\n# # from itertools import combinations\n# # import pandas as pd\n# # import numpy as np\n# from sklearn.model_selection import train_test_split\n# # import torch\n# from torch.utils.data import DataLoader, Dataset\n# # from transformers import TrainingArguments, Trainer, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n# # from trl import PPOTrainer, PPOConfig, DPOTrainer, AutoModelForCausalLMWithValueHead\n# # from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n# from datasets import Dataset as HFDataset\n# from tqdm import tqdm\n# # import wandb","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:52.010825Z","iopub.execute_input":"2024-10-31T12:46:52.011090Z","iopub.status.idle":"2024-10-31T12:46:52.020016Z","shell.execute_reply.started":"2024-10-31T12:46:52.011060Z","shell.execute_reply":"2024-10-31T12:46:52.019159Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# bnb_config = BitsAndBytesConfig(\n# #     load_in_8bit=True,\n#     load_in_4bit=True\n# )","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:52.021111Z","iopub.execute_input":"2024-10-31T12:46:52.021775Z","iopub.status.idle":"2024-10-31T12:46:52.030433Z","shell.execute_reply.started":"2024-10-31T12:46:52.021741Z","shell.execute_reply":"2024-10-31T12:46:52.029559Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"This configuration defines settings for LoRA (Low-Rank Adaptation), a parameter-efficient fine-tuning method that optimizes specific layers in a large model, using the LoraConfig class. Hereâ€™s what each parameter does:\n\n**target_modules**: This specifies which layers in the model will be fine-tuned. Here, it includes layers like \"q_proj\", \"k_proj\", \"v_proj\", and \"o_proj\" (often associated with self-attention operations) as well as \"gate_proj\", \"up_proj\", and \"down_proj\" (often related to feedforward network layers). By targeting only these layers, LoRA reduces the number of parameters being trained, making the process more efficient.\n\n**r**: This represents the rank of the low-rank decomposition in LoRA. A higher rank means more parameters will be used in the decomposition. Here, itâ€™s set to 32, providing a balance between model expressivity and parameter efficiency.\n\n**lora_alpha**: This scaling factor (set to 16) controls the extent to which the LoRA adjustments are scaled. Higher values increase the impact of the LoRA updates on the model, potentially improving learning but also increasing the risk of overfitting.\n\n**lora_dropout**: Set to 0.05, this applies dropout to the LoRA layers. Adding dropout introduces regularization, reducing overfitting by randomly deactivating some parameters during training.\n\n**task_type**: This indicates the type of task the model is being fine-tuned for. Here, itâ€™s set to \"CAUSAL_LM\", meaning causal language modeling, which is common for autoregressive tasks where the model predicts the next token in a sequence.\n\n**inference_mode**: Set to False, this means the configuration is intended for training rather than inference. If True, it would optimize for inference, using the trained LoRA layers without further updates.","metadata":{}},{"cell_type":"code","source":"# peft_config = LoraConfig(\n#     # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n#     r=32,\n#     lora_alpha=16,\n#     lora_dropout=0.05,\n#     task_type=\"CAUSAL_LM\",\n#     inference_mode=False,\n# )","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:52.031554Z","iopub.execute_input":"2024-10-31T12:46:52.032123Z","iopub.status.idle":"2024-10-31T12:46:52.040125Z","shell.execute_reply.started":"2024-10-31T12:46:52.032081Z","shell.execute_reply":"2024-10-31T12:46:52.039387Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# def get_base_model(base_model_name: str, tokenizer: AutoTokenizer, test: bool):\n#     base_model = AutoModelForCausalLM.from_pretrained(\n#         base_model_name,\n#         pad_token_id=tokenizer.pad_token_id,\n#         quantization_config=None if test else bnb_config,\n#         # Higher precision for non-quantized parameters helps training accuracy and doesn't hurt performance\n#         # Lower precision at test time improves speed and only marginally hurts performance\n#         torch_dtype=torch.float16 if test else torch.float32,\n#         device_map={\"\": 0}\n#     )\n#     base_model.config.use_cache = False\n#     base_model.config.pretraining_tp = 1\n#     return base_model","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:52.041901Z","iopub.execute_input":"2024-10-31T12:46:52.042669Z","iopub.status.idle":"2024-10-31T12:46:52.052912Z","shell.execute_reply.started":"2024-10-31T12:46:52.042635Z","shell.execute_reply":"2024-10-31T12:46:52.052065Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# def get_model(base_model_name: str, model_name: Optional[str], pt_model_name: Optional[str],\n#               include_value_head: bool, test: bool, use_gradient_checkpointing: bool = True):\n#     tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n#     tokenizer.pad_token = tokenizer.eos_token\n#     tokenizer.padding_side = \"right\"\n#     model = get_base_model(base_model_name, tokenizer, test)\n#     if test:\n#         # TODO: recommended to merge from quantized model - https://huggingface.co/docs/trl/main/en/dpo_trainer#downsides-to-merging-qlora-before-dpo-approach-2\n#         # If model was adapted on top of pre-trained model, load the pre-trained adapter first\n#         if pt_model_name:\n#             model = PeftModel.from_pretrained(model, pt_model_name).merge_and_unload()\n#         model = PeftModel.from_pretrained(model, model_name).merge_and_unload()\n#     else:\n#         # The pre-trained model serves as the base for the peft model AND as the reference model for KL regularization\n#         # The trl API can disable the adapters on the peft model to recover the reference model\n#         # Thus the reference model needs to be merged and unloaded, and doing it while quantized to save memory\n#         # Will cause \"UserWarning: Merge lora module to 8-bit linear may get different generations due to rounding errors.\"\n#         if pt_model_name:\n#             model = PeftModel.from_pretrained(model, pt_model_name).merge_and_unload()\n#         # Create newly initialized LoRA adapters on top of base model\n#         # Gradient checkpointing can be used to save memory at cost of some time\n#         model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=use_gradient_checkpointing)\n#         model = get_peft_model(model, peft_config)\n#         # Wrap model to add (newly initialized) value head for PPO training\n#         if include_value_head:\n#             model = AutoModelForCausalLMWithValueHead(model).to(device)\n#             model.is_peft_model = True # Tells PPO trainer to disable adapters to recover reference model\n#     return model, tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:52.054261Z","iopub.execute_input":"2024-10-31T12:46:52.054585Z","iopub.status.idle":"2024-10-31T12:46:52.062948Z","shell.execute_reply.started":"2024-10-31T12:46:52.054527Z","shell.execute_reply":"2024-10-31T12:46:52.062152Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"DATA","metadata":{}},{"cell_type":"code","source":"prompt_style = \"\"\"\n    [INST] Generate \"socratic\" guidance as a \"Socratic Guiding Assistant\" to help the User debug their code. The generated guidance must help the User realize their bug. The guidance must not directly reveal the bug fix, and must be coherent with the conversation so far.\n    You are given the following metadata:\n    1. Problem Description and Test Cases (<problem>)\n    2. Student's buggy code (<bug_code>)\n    3. Bug Description (<bug_desc>)\n    4. Bug Fixes (<bug_fixes>)\n    5. Conversation (between User and Assistant) so far (<CONVERSATION>)[/INST]\n\n    <METADATA>\n    {}\n    </METADATA>\n\n    <CONVERSATION>\n    {}\n    {}\n    </CONVERSATION>\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:20:47.789856Z","iopub.execute_input":"2024-10-31T15:20:47.790254Z","iopub.status.idle":"2024-10-31T15:20:47.796367Z","shell.execute_reply.started":"2024-10-31T15:20:47.790218Z","shell.execute_reply":"2024-10-31T15:20:47.795475Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"# def construct_prompt(metadata, input_dialouge):\n#     '''\n#     return fixed prompt \n#     '''\n\n#     fix_prompt = '''\n#     [INST] Generate \"socratic\" guidance as a \"Socratic Guiding Assistant\" to help the User debug their code. The generated guidance must help the User realize their bug. The guidance must not directly reveal the bug fix, and must be coherent with the conversation so far.\n#     You are given the following metadata:\n#     1. Problem Description and Test Cases (<problem>)\n#     2. Student's buggy code (<bug_code>)\n#     3. Bug Description (<bug_desc>)\n#     4. Bug Fixes (<bug_fixes>)\n#     5. Conversation (between User and Assistant) so far (<CONVERSATION>)[/INST]\n\n#     <METADATA>\n#     {}\n#     </METADATA>\n\n#     <CONVERSATION>\n#     {}'''.format(metadata, input_dialouge)\n    \n#     return fix_prompt","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:52.076674Z","iopub.execute_input":"2024-10-31T12:46:52.076922Z","iopub.status.idle":"2024-10-31T12:46:52.085497Z","shell.execute_reply.started":"2024-10-31T12:46:52.076893Z","shell.execute_reply":"2024-10-31T12:46:52.084723Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# input_dialouges_dict = load_dataset(\"json\", data_files=\"/kaggle/input/train-data/input_prompts.json\")\n# problem_metadata_dict = load_dataset(\"json\", data_files=\"/kaggle/input/train-data/problem_metadata.json\")\n# good_outputs_dict = load_dataset(\"json\", data_files=\"/kaggle/input/train-data/good_outputs.json\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:08:45.284387Z","iopub.execute_input":"2024-10-31T15:08:45.284809Z","iopub.status.idle":"2024-10-31T15:08:45.290138Z","shell.execute_reply.started":"2024-10-31T15:08:45.284770Z","shell.execute_reply":"2024-10-31T15:08:45.289136Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# len(problem_metadata_dict)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:08:52.060848Z","iopub.execute_input":"2024-10-31T15:08:52.061226Z","iopub.status.idle":"2024-10-31T15:08:52.066317Z","shell.execute_reply.started":"2024-10-31T15:08:52.061190Z","shell.execute_reply":"2024-10-31T15:08:52.065369Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"import json\ndata_path = os.path.join('/kaggle/input/train-data/')\n\n# input prompts \nwith open(os.path.join(data_path, 'input_prompts.json'), 'r') as infile:\n    input_dialouges_dict = json.load(infile)\n\n# problem metadata\nwith open(os.path.join(data_path, 'problem_metadata.json'), 'r') as infile:\n    problem_metadata_dict = json.load(infile)\n\n# good data \nwith open(os.path.join(data_path, 'good_outputs.json'), 'r') as infile:\n    good_outputs_dict = json.load(infile)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:20:58.870224Z","iopub.execute_input":"2024-10-31T15:20:58.870654Z","iopub.status.idle":"2024-10-31T15:20:58.898106Z","shell.execute_reply.started":"2024-10-31T15:20:58.870614Z","shell.execute_reply":"2024-10-31T15:20:58.897214Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:21:05.991928Z","iopub.execute_input":"2024-10-31T15:21:05.992309Z","iopub.status.idle":"2024-10-31T15:21:05.997705Z","shell.execute_reply.started":"2024-10-31T15:21:05.992269Z","shell.execute_reply":"2024-10-31T15:21:05.996675Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ntexts = []\nfor tr_file, metadata in tqdm(problem_metadata_dict.items(), total=len(problem_metadata_dict)):\n    input_dialouges = input_dialouges_dict[tr_file]\n    good_outputs = good_outputs_dict[tr_file]\n\n    for ctr, dialouge in enumerate(input_dialouges):\n        for good_output in good_outputs[ctr]:\n            text = prompt_style.format(metadata, dialouge, good_output) + EOS_TOKEN\n            texts.append(text)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:28:50.641901Z","iopub.execute_input":"2024-10-31T15:28:50.642756Z","iopub.status.idle":"2024-10-31T15:28:50.667697Z","shell.execute_reply.started":"2024-10-31T15:28:50.642698Z","shell.execute_reply":"2024-10-31T15:28:50.666849Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 134/134 [00:00<00:00, 10389.23it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"texts[2]","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:29:40.602864Z","iopub.execute_input":"2024-10-31T15:29:40.603640Z","iopub.status.idle":"2024-10-31T15:29:40.610383Z","shell.execute_reply.started":"2024-10-31T15:29:40.603595Z","shell.execute_reply":"2024-10-31T15:29:40.609473Z"},"trusted":true},"execution_count":96,"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"'\\n    [INST] Generate \"socratic\" guidance as a \"Socratic Guiding Assistant\" to help the User debug their code. The generated guidance must help the User realize their bug. The guidance must not directly reveal the bug fix, and must be coherent with the conversation so far.\\n    You are given the following metadata:\\n    1. Problem Description and Test Cases (<problem>)\\n    2. Student\\'s buggy code (<bug_code>)\\n    3. Bug Description (<bug_desc>)\\n    4. Bug Fixes (<bug_fixes>)\\n    5. Conversation (between User and Assistant) so far (<CONVERSATION>)[/INST]\\n\\n    <METADATA>\\n    <problem>\\nThe CS110Z course director, unfortunately, was tricked into purchasing a Disney Vacation Club timeshare. The good news about DVC is that it lets you reserve a room at any Disney Resort for one week! The downside, however, is that members have to pay an annual \"maintenance fee\" so that the mouse can keep the property looking good (EVEN when Disney World was closed due to COVID-19 . . . yay).\\n\\nThis year, the maintenance was $623.00. If that isn\\'t bad enough, your course director discovered that maintenance fees aren\\'t fixed! On the contrary, it accues each year at a rate of approximately 1.5%.\\n\\nWrite a Python function called `get_years_until(target_value: float) -> int` that takes a target value as a parameter, and returns the number of years (assuming a fixed interest rate) before the maintenance fee exceeds this value.\\n\\n## Example Cases:\\n```\\nget_years_until(624) => 1\\nget_years_until(1000) => 32\\n```\\n\\n</problem>\\n<bug_code>\\n1. def get_years_until(target_amount):\\n2.\\n3.    current_fee = 623.00\\n4.    years = 0\\n5.    total_fee = current_fee\\n6.\\n7.    while total_fee < target_amount:\\n8.        total_fee = current_fee*(1.015**years)\\n9.        years += 1\\n10.\\n11.    return years\\n</bug_code>\\n<bug_desc>\\nOn line 8, the new total fee is calculated before the number of years is incremented, so the final result will be 1 greater than the correct answer.\\n</bug_desc>\\n<bug_fixes>\\nMove line 9 to be above line 8 and within the while loop.\\n</bug_fixes>\\n    </METADATA>\\n\\n    <CONVERSATION>\\n    User: I\\'m really stuck!\\nAssistant: \\n    Have you read the instructions and made sure you are doing everything?\\n    </CONVERSATION></s>'"},"metadata":{}}]},{"cell_type":"code","source":"def formatting_prompts_func():\n    texts = []\n    for tr_file, metadata in tqdm(problem_metadata_dict.items(), total=len(problem_metadata_dict)):\n        input_dialouges = input_dialouges_dict[tr_file]\n        good_outputs = good_outputs_dict[tr_file]\n\n        for ctr, dialouge in enumerate(input_dialouges):\n            for good_output in good_outputs[ctr]:\n                text = prompt_style.format(metadata, dialouge, good_output) + EOS_TOKEN\n                texts.append(text)\n    return {\"text\": texts}\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:30:58.714488Z","iopub.execute_input":"2024-10-31T15:30:58.715160Z","iopub.status.idle":"2024-10-31T15:30:58.721794Z","shell.execute_reply.started":"2024-10-31T15:30:58.715118Z","shell.execute_reply":"2024-10-31T15:30:58.720912Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"dataset = formatting_prompts_func()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:31:00.677232Z","iopub.execute_input":"2024-10-31T15:31:00.677646Z","iopub.status.idle":"2024-10-31T15:31:00.696272Z","shell.execute_reply.started":"2024-10-31T15:31:00.677605Z","shell.execute_reply":"2024-10-31T15:31:00.695386Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 134/134 [00:00<00:00, 15933.91it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.DataFrame(dataset[\"text\"], columns=[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:32:17.127518Z","iopub.execute_input":"2024-10-31T15:32:17.128269Z","iopub.status.idle":"2024-10-31T15:32:17.134387Z","shell.execute_reply.started":"2024-10-31T15:32:17.128216Z","shell.execute_reply":"2024-10-31T15:32:17.133487Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"# Save DataFrame to CSV format\ncsv_path = \"/kaggle/working/dataset_new.csv\"\ndf.to_csv(csv_path, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:32:29.276422Z","iopub.execute_input":"2024-10-31T15:32:29.277087Z","iopub.status.idle":"2024-10-31T15:32:29.521353Z","shell.execute_reply.started":"2024-10-31T15:32:29.277043Z","shell.execute_reply":"2024-10-31T15:32:29.520556Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:32:31.939141Z","iopub.execute_input":"2024-10-31T15:32:31.939530Z","iopub.status.idle":"2024-10-31T15:32:31.951110Z","shell.execute_reply.started":"2024-10-31T15:32:31.939492Z","shell.execute_reply":"2024-10-31T15:32:31.950116Z"},"trusted":true},"execution_count":104,"outputs":[{"execution_count":104,"output_type":"execute_result","data":{"text/plain":"                                                text\n0  \\n    [INST] Generate \"socratic\" guidance as a...\n1  \\n    [INST] Generate \"socratic\" guidance as a...\n2  \\n    [INST] Generate \"socratic\" guidance as a...\n3  \\n    [INST] Generate \"socratic\" guidance as a...\n4  \\n    [INST] Generate \"socratic\" guidance as a...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\n    [INST] Generate \"socratic\" guidance as a...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\\n    [INST] Generate \"socratic\" guidance as a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\\n    [INST] Generate \"socratic\" guidance as a...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\\n    [INST] Generate \"socratic\" guidance as a...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\n    [INST] Generate \"socratic\" guidance as a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(dataset[\"text\"][10])  # Display the first formatted entry","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:32:37.983295Z","iopub.execute_input":"2024-10-31T15:32:37.983714Z","iopub.status.idle":"2024-10-31T15:32:37.990522Z","shell.execute_reply.started":"2024-10-31T15:32:37.983673Z","shell.execute_reply":"2024-10-31T15:32:37.989356Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"\n    [INST] Generate \"socratic\" guidance as a \"Socratic Guiding Assistant\" to help the User debug their code. The generated guidance must help the User realize their bug. The guidance must not directly reveal the bug fix, and must be coherent with the conversation so far.\n    You are given the following metadata:\n    1. Problem Description and Test Cases (<problem>)\n    2. Student's buggy code (<bug_code>)\n    3. Bug Description (<bug_desc>)\n    4. Bug Fixes (<bug_fixes>)\n    5. Conversation (between User and Assistant) so far (<CONVERSATION>)[/INST]\n\n    <METADATA>\n    <problem>\nThe CS110Z course director, unfortunately, was tricked into purchasing a Disney Vacation Club timeshare. The good news about DVC is that it lets you reserve a room at any Disney Resort for one week! The downside, however, is that members have to pay an annual \"maintenance fee\" so that the mouse can keep the property looking good (EVEN when Disney World was closed due to COVID-19 . . . yay).\n\nThis year, the maintenance was $623.00. If that isn't bad enough, your course director discovered that maintenance fees aren't fixed! On the contrary, it accues each year at a rate of approximately 1.5%.\n\nWrite a Python function called `get_years_until(target_value: float) -> int` that takes a target value as a parameter, and returns the number of years (assuming a fixed interest rate) before the maintenance fee exceeds this value.\n\n## Example Cases:\n```\nget_years_until(624) => 1\nget_years_until(1000) => 32\n```\n\n</problem>\n<bug_code>\n1. def get_years_until(target_amount):\n2.\n3.    current_fee = 623.00\n4.    years = 0\n5.    total_fee = current_fee\n6.\n7.    while total_fee < target_amount:\n8.        total_fee = current_fee*(1.015**years)\n9.        years += 1\n10.\n11.    return years\n</bug_code>\n<bug_desc>\nOn line 8, the new total fee is calculated before the number of years is incremented, so the final result will be 1 greater than the correct answer.\n</bug_desc>\n<bug_fixes>\nMove line 9 to be above line 8 and within the while loop.\n</bug_fixes>\n    </METADATA>\n\n    <CONVERSATION>\n    User: I'm really stuck!\nAssistant: Let me help, do you have any idea what's wrong?\t<alt>What test cases are passing?\t<alt>Have you read the instructions and made sure you are doing everything?\t<alt>Where do you think the problem is?\nUser: I don't know. None of the test cases are passing.\nAssistant: Okay, let's start with the first part of your function. What happens on lines 3, 4, and 5?\t<alt>Let's start by checking the math, what numbers do we start with?\t<alt>How should the calculation work?\t<alt>Could we add some print statements to check the math?\nUser: That's where I set up the variables.\nAssistant: Okay, let's check the loop now, what is it for?\nUser: That's where we calculate the change over all the years.\nAssistant: \n    How do we decide when we want to stop looping?\n    </CONVERSATION></s>\n","output_type":"stream"}]},{"cell_type":"code","source":"# from datasets import Dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"csv\", data_files=\"/kaggle/input/socraticq-new-t/SocraticQ dataset (2).csv\", split = \"train\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:34:17.711468Z","iopub.execute_input":"2024-10-31T15:34:17.712353Z","iopub.status.idle":"2024-10-31T15:34:18.183899Z","shell.execute_reply.started":"2024-10-31T15:34:17.712310Z","shell.execute_reply":"2024-10-31T15:34:18.183132Z"},"trusted":true},"execution_count":106,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48c06aa8546a4ce5814145990554cfa4"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset.column_names)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:34:24.869272Z","iopub.execute_input":"2024-10-31T15:34:24.869913Z","iopub.status.idle":"2024-10-31T15:34:24.876511Z","shell.execute_reply.started":"2024-10-31T15:34:24.869870Z","shell.execute_reply":"2024-10-31T15:34:24.875507Z"},"trusted":true},"execution_count":107,"outputs":[{"name":"stdout","text":"['text']\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset['text'][0]","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:34:26.936361Z","iopub.execute_input":"2024-10-31T15:34:26.936755Z","iopub.status.idle":"2024-10-31T15:34:26.951747Z","shell.execute_reply.started":"2024-10-31T15:34:26.936715Z","shell.execute_reply":"2024-10-31T15:34:26.950883Z"},"trusted":true},"execution_count":108,"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"'\\n    [INST] Generate \"socratic\" guidance as a \"Socratic Guiding Assistant\" to help the User debug their code. The generated guidance must help the User realize their bug. The guidance must not directly reveal the bug fix, and must be coherent with the conversation so far.\\n    You are given the following metadata:\\n    1. Problem Description and Test Cases (<problem>)\\n    2. Student\\'s buggy code (<bug_code>)\\n    3. Bug Description (<bug_desc>)\\n    4. Bug Fixes (<bug_fixes>)\\n    5. Conversation (between User and Assistant) so far (<CONVERSATION>)[/INST]\\n\\n    <METADATA>\\n    <problem>\\nThe CS110Z course director, unfortunately, was tricked into purchasing a Disney Vacation Club timeshare. The good news about DVC is that it lets you reserve a room at any Disney Resort for one week! The downside, however, is that members have to pay an annual \"maintenance fee\" so that the mouse can keep the property looking good (EVEN when Disney World was closed due to COVID-19 . . . yay).\\n\\nThis year, the maintenance was $623.00. If that isn\\'t bad enough, your course director discovered that maintenance fees aren\\'t fixed! On the contrary, it accues each year at a rate of approximately 1.5%.\\n\\nWrite a Python function called `get_years_until(target_value: float) -> int` that takes a target value as a parameter, and returns the number of years (assuming a fixed interest rate) before the maintenance fee exceeds this value.\\n\\n## Example Cases:\\n```\\nget_years_until(624) => 1\\nget_years_until(1000) => 32\\n```\\n\\n</problem>\\n<bug_code>\\n1. def get_years_until(target_amount):\\n2.\\n3.    current_fee = 623.00\\n4.    years = 0\\n5.    total_fee = current_fee\\n6.\\n7.    while total_fee < target_amount:\\n8.        total_fee = current_fee*(1.015**years)\\n9.        years += 1\\n10.\\n11.    return years\\n</bug_code>\\n<bug_desc>\\nOn line 8, the new total fee is calculated before the number of years is incremented, so the final result will be 1 greater than the correct answer.\\n</bug_desc>\\n<bug_fixes>\\nMove line 9 to be above line 8 and within the while loop.\\n</bug_fixes>\\n    </METADATA>\\n\\n    <CONVERSATION>\\n    User: I\\'m really stuck!\\nAssistant: \\n    Let me help, do you have any idea what\\'s wrong?\\n    </CONVERSATION></s>'"},"metadata":{}}]},{"cell_type":"code","source":"# EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n# # Modify formatting function to support batched=True\n# def formatting_prompts_func(batch):\n#     input_dialouges_batch = batch['input_dialouges']\n#     problem_metadata_batch = batch['problem_metadata']\n    \n#     texts = []\n#     for idx, input_dialouge in enumerate(input_dialouges_batch):\n#         text = prompt_style.format(problem_metadata_batch[idx], input_dialouges_batch[idx]) + EOS_TOKEN\n#         texts.append(text)\n#         # Optional: You can print or log the index if needed\n# #         print(f\"Processed item {idx}\")  # Remove this line in production if not needed\n#     return {\"text\": texts}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Prepare data in a format compatible with Dataset\n# data = {\n#     \"input_dialouges\": input_dialouges_dict,\n#     \"problem_metadata\": problem_metadata_dict,\n# }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Create Dataset from data\n# dataset = Dataset.from_dict(data)\n\n# # Apply formatting function with batched=True\n# dataset = dataset.map(\n#     formatting_prompts_func,\n#     batched=True\n# )\n\n# # Verify the first entry\n# print(dataset[\"text\"][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def construct_data(split_path: str):\n#     '''\n#     create a list of dictionaries for SFT\n#     '''\n    \n    \n\n# #     if preference_data:\n# #         # preference data\n# #         with open(os.path.join(data_path, 'preference_data.json'), 'r') as infile:\n# #             preference_data_dict = json.load(infile)\n    \n\n#     all_data = []\n\n#     for tr_file, metadata in tqdm(problem_metadata_dict.items(), total=len(problem_metadata_dict)):\n#         input_dialouges = input_dialouges_dict[tr_file]\n#         good_outputs_list = good_outputs_dict[tr_file]\n#         for ctr, dialouge in enumerate(input_dialouges):\n#             # construct prompt\n#             fix_prompt = construct_prompt(metadata, dialouge)\n# #             if not preference_data:\n                \n# #                 if split_path == 'testset':\n# #                     all_data.append({'prompt': fix_prompt, 'output': str(good_outputs_list[ctr])})\n# #                 else:\n#             for good_output in good_outputs_list[ctr]:\n#                 # append to all data\n#                 all_data.append({'prompt': fix_prompt, 'output': good_output+'</CONVERSATION>'})\n# #             else:\n# #                 preference_data_list = preference_data_dict[tr_file]\n# #                 # create dataset for DPO training\n# #                 for preference_tuples in preference_data_list[ctr]:\n# #                     # append to all data\n# #                     all_data.append({'prompt': fix_prompt, 'chosen': str(preference_tuples[0])+'</CONVERSATION>', 'rejected': str(preference_tuples[1])+'</CONVERSATION>'})\n                \n    \n#     return all_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def construct_data(split_path: str, preference_data=False):\n#     '''\n#     create a list of dictionaries for SFT\n#     '''\n    \n    \n#     data_path = os.path.join('/kaggle/input/', split_path)\n#     # input prompts \n#     with open(os.path.join(data_path, 'input_prompts.json'), 'r') as infile:\n#         input_dialouges_dict = json.load(infile)\n#     # problem metadata\n#     with open(os.path.join(data_path, 'problem_metadata.json'), 'r') as infile:\n#         problem_metadata_dict = json.load(infile)\n#     # good data \n#     with open(os.path.join(data_path, 'good_outputs.json'), 'r') as infile:\n#         good_outputs_dict = json.load(infile)\n#     if preference_data:\n#         # preference data\n#         with open(os.path.join(data_path, 'preference_data.json'), 'r') as infile:\n#             preference_data_dict = json.load(infile)\n    \n\n#     all_data = []\n\n#     for tr_file, metadata in tqdm(problem_metadata_dict.items(), total=len(problem_metadata_dict)):\n#         input_dialouges = input_dialouges_dict[tr_file]\n#         good_outputs_list = good_outputs_dict[tr_file]\n#         for ctr, dialouge in enumerate(input_dialouges):\n#             # construct prompt\n#             fix_prompt = construct_prompt(metadata, dialouge)\n#             if not preference_data:\n                \n#                 if split_path == 'testset':\n#                     all_data.append({'prompt': fix_prompt, 'output': str(good_outputs_list[ctr])})\n#                 else:\n#                     for good_output in good_outputs_list[ctr]:\n#                         # append to all data\n#                         all_data.append({'prompt': fix_prompt, 'output': good_output+'</CONVERSATION>'})\n#             else:\n#                 preference_data_list = preference_data_dict[tr_file]\n#                 # create dataset for DPO training\n#                 for preference_tuples in preference_data_list[ctr]:\n#                     # append to all data\n#                     all_data.append({'prompt': fix_prompt, 'chosen': str(preference_tuples[0])+'</CONVERSATION>', 'rejected': str(preference_tuples[1])+'</CONVERSATION>'})\n                \n    \n#     return all_data","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:02:06.247247Z","iopub.execute_input":"2024-10-31T11:02:06.247979Z","iopub.status.idle":"2024-10-31T11:02:06.261260Z","shell.execute_reply.started":"2024-10-31T11:02:06.247937Z","shell.execute_reply":"2024-10-31T11:02:06.260393Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# class QGSFTDataset(Dataset):\n#     '''\n#     QG Dataset\n#     '''\n#     def __init__(self, data: list):\n#         self.data = data\n#         self.column_names = list(data[0].keys())\n    \n#     def __getitem__(self, index: int):\n#         return self.data[index]\n\n#     def __len__(self):\n#         return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:11:05.947684Z","iopub.execute_input":"2024-10-31T11:11:05.948459Z","iopub.status.idle":"2024-10-31T11:11:05.955073Z","shell.execute_reply.started":"2024-10-31T11:11:05.948415Z","shell.execute_reply":"2024-10-31T11:11:05.954098Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# class QGSFTCollator:\n#     def __init__(self, tokenizer, test: bool):\n#         self.tokenizer = tokenizer\n#         self.test = test\n\n#     def __call__(self, batch):\n#         all_prompts = [sample[\"prompt\"] for sample in batch]\n#         prompts_tokenized = self.tokenizer(all_prompts, return_tensors=\"pt\", padding=True)\n#         if self.test:\n#             return {\n#                 \"input_ids\": prompts_tokenized.input_ids.to(device),\n#                 \"attention_mask\": prompts_tokenized.attention_mask.to(device),\n#                 \"meta_data\": batch\n#             }\n\n#         # TODO: might be worth debugging this\n#         all_inputs = [sample[\"prompt\"] + sample[\"output\"] + self.tokenizer.eos_token for sample in batch]\n#         inputs_tokenized = self.tokenizer(all_inputs, return_tensors=\"pt\", padding=True)\n#         prompt_lens = prompts_tokenized.attention_mask.sum(dim=1)\n#         labels = inputs_tokenized.input_ids.clone()\n#         padding_mask = torch.arange(labels.shape[1]).repeat(labels.shape[0], 1) < prompt_lens.unsqueeze(1)\n#         labels[padding_mask] = -100\n#         labels = labels.masked_fill(inputs_tokenized.attention_mask == 0, -100)\n#         return {\n#             \"input_ids\": inputs_tokenized.input_ids,\n#             \"attention_mask\": inputs_tokenized.attention_mask,\n#             \"labels\": labels\n#         }","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:02:16.506847Z","iopub.execute_input":"2024-10-31T11:02:16.507736Z","iopub.status.idle":"2024-10-31T11:02:16.517381Z","shell.execute_reply.started":"2024-10-31T11:02:16.507696Z","shell.execute_reply":"2024-10-31T11:02:16.516529Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# class QGDPODataset(Dataset):\n#     '''\n#     QG Dataset\n#     '''\n#     def __init__(self, data: list):\n#         self.data = data\n    \n#     def __getitem__(self, index: int):\n#         return self.data[index]\n\n#     def __len__(self):\n#         return len(self.data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TRAINING","metadata":{}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r=16, \n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n    lora_alpha=16,\n    lora_dropout=0, \n    bias=\"none\", \n   \n    use_gradient_checkpointing=\"unsloth\", \n    random_state=3407,\n    use_rslora=False, \n    loftq_config=None,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:34:36.399278Z","iopub.execute_input":"2024-10-31T15:34:36.399687Z","iopub.status.idle":"2024-10-31T15:34:36.409271Z","shell.execute_reply.started":"2024-10-31T15:34:36.399649Z","shell.execute_reply":"2024-10-31T15:34:36.408413Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stderr","text":"Unsloth: Already have LoRA adapters! We shall skip this step.\n","output_type":"stream"}]},{"cell_type":"code","source":"# def get_training_args(model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb):\n#     return TrainingArguments(\n#         output_dir=model_name,\n#         num_train_epochs=epochs,\n#         learning_rate=lr,\n#         weight_decay=wd,\n#         max_grad_norm=max_grad_norm or None,\n#         per_device_train_batch_size=batch_size,\n#         gradient_accumulation_steps=grad_accum_steps,\n#         per_device_eval_batch_size=batch_size * 2,\n#         eval_accumulation_steps=4,\n#         warmup_ratio=0.1,\n#         eval_strategy=\"epoch\",\n#         save_strategy=\"epoch\",\n#         save_total_limit=1,\n#         load_best_model_at_end=True,\n#         metric_for_best_model=\"loss\",\n#         greater_is_better=False,\n#         remove_unused_columns=False,\n#         logging_steps=1,               # Log training loss every batch\n#         log_level='info',\n#         report_to=\"wandb\" if wandb else \"none\"\n#     )","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:02:32.931873Z","iopub.execute_input":"2024-10-31T11:02:32.932756Z","iopub.status.idle":"2024-10-31T11:02:32.939908Z","shell.execute_reply.started":"2024-10-31T11:02:32.932715Z","shell.execute_reply":"2024-10-31T11:02:32.938882Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# all_train_data = construct_data(split_path='train-data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_train_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data, val_data = train_test_split(all_train_data, test_size=0.2, random_state=37)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_set = QGSFTDataset(train_data)\n# train_set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:35:19.063579Z","iopub.execute_input":"2024-10-31T15:35:19.063945Z","iopub.status.idle":"2024-10-31T15:35:21.931023Z","shell.execute_reply.started":"2024-10-31T15:35:19.063912Z","shell.execute_reply":"2024-10-31T15:35:21.930031Z"},"trusted":true},"execution_count":110,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/1912 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ac482dfc3f9496fa372c06024907769"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:37:18.536072Z","iopub.execute_input":"2024-10-31T15:37:18.537009Z","iopub.status.idle":"2024-10-31T15:37:18.542162Z","shell.execute_reply.started":"2024-10-31T15:37:18.536966Z","shell.execute_reply":"2024-10-31T15:37:18.541229Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:35:23.967883Z","iopub.execute_input":"2024-10-31T15:35:23.968782Z","iopub.status.idle":"2024-10-31T15:36:15.596281Z","shell.execute_reply.started":"2024-10-31T15:35:23.968736Z","shell.execute_reply":"2024-10-31T15:36:15.594364Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stdout","text":"**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers, TRL and Unsloth!\n`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n","output_type":"stream"},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 1,912 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 60\n \"-____-\"     Number of trainable parameters = 41,943,040\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/60 : < :, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[111], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m<string>:156\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n","File \u001b[0;32m<string>:363\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3324\u001b[0m ):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3363\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3362\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3363\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:820\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:808\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py:1044\u001b[0m, in \u001b[0;36mPeftModelForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPeftModelForCausalLM_fast_forward\u001b[39m(\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1043\u001b[0m ):\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/mistral.py:220\u001b[0m, in \u001b[0;36mMistralForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m LlamaModel_fast_forward_inference(\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    214\u001b[0m         input_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m attention_mask,\n\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    234\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py:806\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 806\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    816\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py:506\u001b[0m, in \u001b[0;36mLlamaDecoderLayer_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    505\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m fast_rms_layernorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm, hidden_states)\n\u001b[0;32m--> 506\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/kernels/fast_lora.py:162\u001b[0m, in \u001b[0;36mapply_lora_mlp_swiglu\u001b[0;34m(self, X, inplace)\u001b[0m\n\u001b[1;32m    160\u001b[0m upW,     upW_quant,   upA,   upB,   upS \u001b[38;5;241m=\u001b[39m get_lora_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m  up_proj)\n\u001b[1;32m    161\u001b[0m downW, downW_quant, downA, downB, downS \u001b[38;5;241m=\u001b[39m get_lora_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj)\n\u001b[0;32m--> 162\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mLoRA_MLP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mgateW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mupW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[43mupW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mupB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mupS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdownW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mswiglu_fg_kernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswiglu_DWf_DW_dfg_kernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                     \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:465\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fwd_used_autocast \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled(device_type)\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     autocast_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled(device_type)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/kernels/fast_lora.py:78\u001b[0m, in \u001b[0;36mLoRA_MLP.forward\u001b[0;34m(ctx, X, gateW, gateW_quant, gateA, gateB, gateS, upW, upW_quant, upA, upB, upS, downW, downW_quant, downA, downB, downS, _forward_function, _backward_function, inplace)\u001b[0m\n\u001b[1;32m     76\u001b[0m g \u001b[38;5;241m=\u001b[39m matmul_lora(X,   upW,   upW_quant,   upA,   upB,   upS)\n\u001b[1;32m     77\u001b[0m h \u001b[38;5;241m=\u001b[39m _forward_function(e, g)\n\u001b[0;32m---> 78\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[43mmatmul_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m ctx\u001b[38;5;241m.\u001b[39mcustom_saved_tensors \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     81\u001b[0m     gateW, gateW_quant, gateS,\n\u001b[1;32m     82\u001b[0m     upW, upW_quant, upS,\n\u001b[1;32m     83\u001b[0m     downW, downW_quant, downS,\n\u001b[1;32m     84\u001b[0m     _backward_function,\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     86\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(gateA, gateB, upA, upB, downA, downB,\n\u001b[1;32m     87\u001b[0m                       X, e, g)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/kernels/utils.py:396\u001b[0m, in \u001b[0;36mmatmul_lora\u001b[0;34m(X, W, W_quant, A, B, s, out)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatmul_lora\u001b[39m(X, W, W_quant, A, B, s, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    395\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 396\u001b[0m     W \u001b[38;5;241m=\u001b[39m \u001b[43mfast_dequantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_quant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    399\u001b[0m         batch, seq_len, d \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/kernels/utils.py:138\u001b[0m, in \u001b[0;36mfast_dequantize\u001b[0;34m(W, quant_state, out)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Create weight matrix\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(out\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m shape)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 113.12 MiB is free. Process 2189 has 15.77 GiB memory in use. Of the allocated memory 14.71 GiB is allocated by PyTorch, and 763.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 113.12 MiB is free. Process 2189 has 15.77 GiB memory in use. Of the allocated memory 14.71 GiB is allocated by PyTorch, and 763.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}]},{"cell_type":"code","source":"new_model_online = \"vihidun/unsloth_mistral-new-7b-bnb-4bit\"\nnew_model_local = \"mistral-new-7b-bnb-4bit\"\nmodel.save_pretrained(new_model_local) # Local saving\ntokenizer.save_pretrained(new_model_local) # Local saving","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:36:15.597243Z","iopub.status.idle":"2024-10-31T15:36:15.597608Z","shell.execute_reply.started":"2024-10-31T15:36:15.597420Z","shell.execute_reply":"2024-10-31T15:36:15.597438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(new_model_online) # Online saving\ntokenizer.push_to_hub(new_model_online) # Online saving","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:36:15.598883Z","iopub.status.idle":"2024-10-31T15:36:15.599213Z","shell.execute_reply.started":"2024-10-31T15:36:15.599046Z","shell.execute_reply":"2024-10-31T15:36:15.599063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:36:15.600206Z","iopub.status.idle":"2024-10-31T15:36:15.600593Z","shell.execute_reply.started":"2024-10-31T15:36:15.600380Z","shell.execute_reply":"2024-10-31T15:36:15.600397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"Vihidun/unsloth_mistral-new-7b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:36:15.602053Z","iopub.status.idle":"2024-10-31T15:36:15.602409Z","shell.execute_reply.started":"2024-10-31T15:36:15.602214Z","shell.execute_reply":"2024-10-31T15:36:15.602231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# alpaca_prompt = Copied from above\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    prompt_style.format(\n        \"<problem>\\nWrite a function `factorial(n:int) -> int` that computes the factorial n! of a natural number n, which is defined mathematically as:\\n\\n$0! = 1$\\n$n! = n \\\\times (n - 1)!$\\n\\nAdditionally, if the input integer n is negative the function should return 0.\\n\\n## Example Cases:\\n```\\nfactorial(-1) => 0\\nfactorial(0) => 1\\nfactorial(1) => 1\\nfactorial(2) => 2\\nfactorial(3) => 6\\nfactorial(4) => 24\\nfactorial(5) => 120\\n```\\n</problem>\\n<bug_code>\\n1. def factorial(n):\\n2.        if n < 0:\\n3.                return 0\\n4.        fact = 1\\n5.        for i in range(n):\\n6.                fact = fact * i\\n7.        return fact\\n</bug_code>\\n<bug_desc>\\nOn line 6, `fact` is multiplied with 0 in the first iteration of the for loop. Consequently, at every iteration fact stays equal with 0 instead of being updated to be equal with factorial of `(i + 1)`. Therefore, the function will return 0, irrespective of n\\n</bug_desc>\\n<bug_fixes>\\nReplace `i` with `(i + 1)` in line 6.\\nReplace `range(n)` with `range(1, n + 1)` in line 5.\\n</bug_fixes>\", \n        \"User: Hi! I implemented the factorial function but it doesn\\u2019t work and I do not know why. Can you help?\\nAssistant: \", \n        \"\"\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\nmodel_output = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:36:22.263994Z","iopub.execute_input":"2024-10-31T15:36:22.264679Z","iopub.status.idle":"2024-10-31T15:36:23.543320Z","shell.execute_reply.started":"2024-10-31T15:36:22.264636Z","shell.execute_reply":"2024-10-31T15:36:23.541425Z"},"trusted":true},"execution_count":112,"outputs":[{"name":"stdout","text":"<s> \n    [INST] Generate \"socratic\" guidance as a \"Socratic Guiding Assistant\" to help the User debug their code. The generated guidance must help the User realize their bug. The guidance must not directly reveal the bug fix, and must be coherent with the conversation so far.\n    You are given the following metadata:\n    1. Problem Description and Test Cases (<problem>)\n    2. Student's buggy code (<bug_code>)\n    3. Bug Description (<bug_desc>)\n    4. Bug Fixes (<bug_fixes>)\n    5. Conversation (between User and Assistant) so far (<CONVERSATION>)[/INST]\n\n    <METADATA>\n    <problem>\nWrite a function `factorial(n:int) -> int` that computes the factorial n! of a natural number n, which is defined mathematically as:\n\n$0! = 1$\n$n! = n \\times (n - 1)!$\n\nAdditionally, if the input integer n is negative the function should return 0.\n\n## Example Cases:\n```\nfactorial(-1) => 0\nfactorial(0) => 1\nfactorial(1) => 1\nfactorial(2) => 2\nfactorial(3) => 6\nfactorial(4) => 24\nfactorial(5) => 120\n```\n</problem>\n<bug_code>\n1. def factorial(n):\n2.        if n < 0:\n3.                return 0\n4.        fact = 1\n5.        for i in range(n):\n6.                fact = fact * i\n7.        return fact\n</bug_code>\n<bug_desc>\nOn line 6, `fact` is multiplied with 0 in the first iteration of the for loop. Consequently, at every iteration fact stays equal with 0 instead of being updated to be equal with factorial of `(i + 1)`. Therefore, the function will return 0, irrespective of n\n</bug_desc>\n<bug_fixes>\nReplace `i` with `(i + 1)` in line 6.\nReplace `range(n)` with `range(1, n + 1)` in line 5.\n</bug_fixes>\n    </METADATA>\n\n    <CONVERSATION>\n    User: Hi! I implemented the factorial function but it doesnâ€™t work and I do not know why. Can you help?\nAssistant: \n    \n    ","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[112], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextStreamer\n\u001b[1;32m     13\u001b[0m text_streamer \u001b[38;5;241m=\u001b[39m TextStreamer(tokenizer)\n\u001b[0;32m---> 14\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_streamer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py:1413\u001b[0m, in \u001b[0;36m_wrap_fast_inference.<locals>._fast_generate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;66;03m# Set pad token\u001b[39;00m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;66;03m# old_pad_token_id = getattr(model.config, \"pad_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;66;03m# old_eos_token_id = getattr(model.config, \"eos_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_eos_token_id\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \n\u001b[1;32m   1411\u001b[0m \u001b[38;5;66;03m# Autocasted\u001b[39;00m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type \u001b[38;5;241m=\u001b[39m device_type, dtype \u001b[38;5;241m=\u001b[39m dtype):\n\u001b[0;32m-> 1413\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;66;03m# Revert\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_pad_token_id\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m \n\u001b[1;32m   1419\u001b[0m \u001b[38;5;66;03m# Unset a flag for generation!\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1704\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1703\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1704\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1706\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2982\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2979\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2982\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2985\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/mistral.py:220\u001b[0m, in \u001b[0;36mMistralForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m LlamaModel_fast_forward_inference(\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    214\u001b[0m         input_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m attention_mask,\n\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    234\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py:806\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 806\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    816\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py:506\u001b[0m, in \u001b[0;36mLlamaDecoderLayer_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    505\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m fast_rms_layernorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm, hidden_states)\n\u001b[0;32m--> 506\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/kernels/fast_lora.py:162\u001b[0m, in \u001b[0;36mapply_lora_mlp_swiglu\u001b[0;34m(self, X, inplace)\u001b[0m\n\u001b[1;32m    160\u001b[0m upW,     upW_quant,   upA,   upB,   upS \u001b[38;5;241m=\u001b[39m get_lora_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m  up_proj)\n\u001b[1;32m    161\u001b[0m downW, downW_quant, downA, downB, downS \u001b[38;5;241m=\u001b[39m get_lora_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj)\n\u001b[0;32m--> 162\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mLoRA_MLP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mgateW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mupW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[43mupW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mupB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mupS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdownW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mswiglu_fg_kernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswiglu_DWf_DW_dfg_kernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                     \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:465\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fwd_used_autocast \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled(device_type)\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     autocast_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled(device_type)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/kernels/fast_lora.py:75\u001b[0m, in \u001b[0;36mLoRA_MLP.forward\u001b[0;34m(ctx, X, gateW, gateW_quant, gateA, gateB, gateS, upW, upW_quant, upA, upB, upS, downW, downW_quant, downA, downB, downS, _forward_function, _backward_function, inplace)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;129m@torch_amp_custom_fwd\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, X : torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m             _forward_function, _backward_function,\n\u001b[1;32m     72\u001b[0m             inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,):\n\u001b[1;32m     73\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m---> 75\u001b[0m     e \u001b[38;5;241m=\u001b[39m \u001b[43mmatmul_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     g \u001b[38;5;241m=\u001b[39m matmul_lora(X,   upW,   upW_quant,   upA,   upB,   upS)\n\u001b[1;32m     77\u001b[0m     h \u001b[38;5;241m=\u001b[39m _forward_function(e, g)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/kernels/utils.py:396\u001b[0m, in \u001b[0;36mmatmul_lora\u001b[0;34m(X, W, W_quant, A, B, s, out)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatmul_lora\u001b[39m(X, W, W_quant, A, B, s, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    395\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 396\u001b[0m     W \u001b[38;5;241m=\u001b[39m \u001b[43mfast_dequantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_quant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    399\u001b[0m         batch, seq_len, d \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/kernels/utils.py:138\u001b[0m, in \u001b[0;36mfast_dequantize\u001b[0;34m(W, quant_state, out)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Create weight matrix\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(out\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m shape)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 113.12 MiB is free. Process 2189 has 15.77 GiB memory in use. Of the allocated memory 14.72 GiB is allocated by PyTorch, and 746.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 113.12 MiB is free. Process 2189 has 15.77 GiB memory in use. Of the allocated memory 14.72 GiB is allocated by PyTorch, and 746.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}]},{"cell_type":"code","source":"# from trl import SFTTrainer\n# from transformers import TrainingArguments\n# from unsloth import is_bfloat16_supported\n\n# def sft(base_model, model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb):\n    \n#     # construct data\n#     print('#### Constructing Data ####')\n#     all_train_data = construct_data(split_path='train-data')\n    \n#     print(\"#### Splitting Data ####\")\n#     # split into train and val (80-20)\n#     train_data, val_data = train_test_split(all_train_data, test_size=0.2, random_state=37)\n    \n#     trainer = SFTTrainer(\n#         model = model,\n\n#         train_dataset=QGSFTDataset(train_data),\n#         eval_dataset=QGSFTDataset(val_data),\n#         data_collator=QGSFTCollator(tokenizer, False),\n        \n# #         dataset_text_field = \"text\",\n# #         max_seq_length = max_seq_length,\n# #         dataset_num_proc = 2,\n#         args=get_training_args(model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb),\n\n#     )\n    \n#     print(\"#### train model ####\")\n#     trainer.train()\n    \n#     print(\"#### save model ####\")\n#     model_save_path = f\"/kaggle/working/{model_name}_final_model\"\n#     trainer.save_model(model_save_path)\n    \n#     print(\"#### Log model to WandB ####\")\n#     # Create and log a WandB artifact for the trained model\n#     artifact = wandb.Artifact(name=model_name, type=\"model\")\n#     artifact.add_dir(model_save_path)  # Add the model directory to the artifact\n#     wandb.log_artifact(artifact)  # Log the artifact to WandB\n    \n#     print(\"Model saved and logged to WandB successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:02:56.137037Z","iopub.execute_input":"2024-10-31T11:02:56.138072Z","iopub.status.idle":"2024-10-31T11:02:56.149796Z","shell.execute_reply.started":"2024-10-31T11:02:56.138019Z","shell.execute_reply":"2024-10-31T11:02:56.148930Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# def sft(base_model, model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb):\n# #     assert args.model_name\n\n\n#     # construct data\n#     print('#### Constructing Data ####')\n#     all_train_data = construct_data(split_path='train-data')\n    \n#     print(\"#### Splitting Data ####\")\n#     # split into train and val (80-20)\n#     train_data, val_data = train_test_split(all_train_data, test_size=0.2, random_state=37)\n    \n#     print('#### get model and tokenizer ####')\n#     model, tokenizer = get_model(base_model, None, None, False, False)\n    \n#     print(\"#### Run training configuration ####\")\n#     trainer = Trainer(\n#         model=model,\n#         args=get_training_args(model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb),\n#         train_dataset=QGSFTDataset(train_data),\n#         eval_dataset=QGSFTDataset(val_data),\n#         data_collator=QGSFTCollator(tokenizer, False)\n#     )\n    \n#     print(\"#### train model ####\")\n#     trainer.train()\n    \n#     print(\"#### save model ####\")\n#     trainer.save_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SANITY CHECKS","metadata":{}},{"cell_type":"code","source":"# def check_max_token_len(base_model, split_type='train'):\n#     # load data \n#     if split_type == 'train':\n#         data = construct_data(split_path='train-data')\n#     elif split_type == 'testset':\n#         data = construct_data(split_path='test-data')\n\n#     tokenizer = AutoTokenizer.from_pretrained(base_model)\n#     tokenizer.pad_token = tokenizer.eos_token\n#     tokenizer.padding_side = \"right\"\n#     max_len_full_prompt = 0\n#     max_len_full_output = 0\n#     for sample in tqdm(data, total = len(data)):\n#         input_sample = sample['prompt'] + sample['output'] + tokenizer.eos_token\n#         tokenized = tokenizer.encode(input_sample)\n#         tokenize_output = tokenizer.encode(sample['output'])\n#         max_len_full_prompt = max(max_len_full_prompt, len(tokenized))\n#         max_len_full_output = max(max_len_full_output, len(tokenize_output))\n    \n#     print('#### Split Type: {} ####'.format(split_type))\n#     print(f\"Maximum tokenized length: {max_len_full_prompt}\")\n#     print(f\"Maximum tokenized length: {max_len_full_output}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def perform_sanity_checks(base_model):\n#     # check maximum tokenized length of the inputs \n#     check_max_token_len(base_model, split_type='train')\n#     check_max_token_len(base_model, split_type='testset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MAIN","metadata":{}},{"cell_type":"code","source":"# python finetune/sft_dpo.py --sft --base_model codellama/CodeLlama-7b-Instruct-hf --model_name codellama_sft_b2 --batch_size 2 --grad_accum_steps 32 --epochs 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # base_model = \"codellama/CodeLlama-7b-Instruct-hf\"\n# # base_model = \"microsoft/Phi-3.5-mini-instruct\"\n# base_model = \"unsloth/mistral-7b-bnb-4bit\"\n# # model_name = \"codellama_sft_b2\"\n# model_name = \"mistral-7b-bnb-4bit_sft_b2\"\n# batch_size = 2\n# grad_accum_steps = 32\n# epochs = 5\n# max_grad_norm = 1.0\n# lr = 3e-5\n# wd = 0.0\n# wandb = \"wandb\"","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:18:50.002174Z","iopub.execute_input":"2024-10-31T13:18:50.003154Z","iopub.status.idle":"2024-10-31T13:18:50.008433Z","shell.execute_reply.started":"2024-10-31T13:18:50.003109Z","shell.execute_reply":"2024-10-31T13:18:50.007707Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# torch.cuda.empty_cache()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sft(base_model, model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:18:53.262383Z","iopub.execute_input":"2024-10-31T13:18:53.263029Z","iopub.status.idle":"2024-10-31T13:18:53.343460Z","shell.execute_reply.started":"2024-10-31T13:18:53.262990Z","shell.execute_reply":"2024-10-31T13:18:53.342298Z"},"trusted":true},"execution_count":58,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msft\u001b[49m(base_model, model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb)\n","\u001b[0;31mNameError\u001b[0m: name 'sft' is not defined"],"ename":"NameError","evalue":"name 'sft' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import shutil\n\n# Create a zip file of the directory\nshutil.make_archive('/kaggle/working/wandb', 'zip', '/kaggle/working/wandb')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T14:48:31.298973Z","iopub.execute_input":"2024-10-31T14:48:31.299681Z","iopub.status.idle":"2024-10-31T14:48:31.332363Z","shell.execute_reply.started":"2024-10-31T14:48:31.299641Z","shell.execute_reply":"2024-10-31T14:48:31.331512Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/wandb.zip'"},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Display a download link\nFileLink(r'/kaggle/working/outputs.zip')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T14:44:18.179549Z","iopub.execute_input":"2024-10-31T14:44:18.180406Z","iopub.status.idle":"2024-10-31T14:44:18.188016Z","shell.execute_reply.started":"2024-10-31T14:44:18.180361Z","shell.execute_reply":"2024-10-31T14:44:18.187120Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/outputs.zip","text/html":"<a href='/kaggle/working/outputs.zip' target='_blank'>/kaggle/working/outputs.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import display, Markdown\n\nFastLanguageModel.for_inference(model)\ninputs = tokenizer(\n    [\n        prompt_style.format(\n            \"<problem>\\nWrite a function `factorial(n:int) -> int` that computes the factorial n! of a natural number n, which is defined mathematically as:\\n\\n$0! = 1$\\n$n! = n \\\\times (n - 1)!$\\n\\nAdditionally, if the input integer n is negative the function should return 0.\\n\\n## Example Cases:\\n```\\nfactorial(-1) => 0\\nfactorial(0) => 1\\nfactorial(1) => 1\\nfactorial(2) => 2\\nfactorial(3) => 6\\nfactorial(4) => 24\\nfactorial(5) => 120\\n```\\n</problem>\\n<bug_code>\\n1. def factorial(n):\\n2.        if n < 0:\\n3.                return 0\\n4.        fact = 1\\n5.        for i in range(n):\\n6.                fact = fact * i\\n7.        return fact\\n</bug_code>\\n<bug_desc>\\nOn line 6, `fact` is multiplied with 0 in the first iteration of the for loop. Consequently, at every iteration fact stays equal with 0 instead of being updated to be equal with factorial of `(i + 1)`. Therefore, the function will return 0, irrespective of n\\n</bug_desc>\\n<bug_fixes>\\nReplace `i` with `(i + 1)` in line 6.\\nReplace `range(n)` with `range(1, n + 1)` in line 5.\\n</bug_fixes>\", \n            \"User: Hi! I implemented the factorial function but it doesn\\u2019t work and I do not know why. Can you help?\\nAssistant: \", \n        )\n    ],\n    return_tensors=\"pt\",\n).to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=512,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\n# Markdown(response[0].split(\"\\n\\n### Response:\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-10-31T14:57:59.317728Z","iopub.execute_input":"2024-10-31T14:57:59.318131Z","iopub.status.idle":"2024-10-31T14:58:00.817902Z","shell.execute_reply.started":"2024-10-31T14:57:59.318096Z","shell.execute_reply":"2024-10-31T14:58:00.816961Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"prompt = prompt_style.format(\n            \"<problem>\\nWrite a function `factorial(n:int) -> int` that computes the factorial n! of a natural number n, which is defined mathematically as:\\n\\n$0! = 1$\\n$n! = n \\\\times (n - 1)!$\\n\\nAdditionally, if the input integer n is negative the function should return 0.\\n\\n## Example Cases:\\n```\\nfactorial(-1) => 0\\nfactorial(0) => 1\\nfactorial(1) => 1\\nfactorial(2) => 2\\nfactorial(3) => 6\\nfactorial(4) => 24\\nfactorial(5) => 120\\n```\\n</problem>\\n<bug_code>\\n1. def factorial(n):\\n2.        if n < 0:\\n3.                return 0\\n4.        fact = 1\\n5.        for i in range(n):\\n6.                fact = fact * i\\n7.        return fact\\n</bug_code>\\n<bug_desc>\\nOn line 6, `fact` is multiplied with 0 in the first iteration of the for loop. Consequently, at every iteration fact stays equal with 0 instead of being updated to be equal with factorial of `(i + 1)`. Therefore, the function will return 0, irrespective of n\\n</bug_desc>\\n<bug_fixes>\\nReplace `i` with `(i + 1)` in line 6.\\nReplace `range(n)` with `range(1, n + 1)` in line 5.\\n</bug_fixes>\", \n            \"User: Hi! I implemented the factorial function but it doesn\\u2019t work and I do not know why. Can you help?\\nAssistant: \", \n        )\nprint(prompt)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T14:57:40.659440Z","iopub.execute_input":"2024-10-31T14:57:40.659834Z","iopub.status.idle":"2024-10-31T14:57:40.667156Z","shell.execute_reply.started":"2024-10-31T14:57:40.659796Z","shell.execute_reply":"2024-10-31T14:57:40.666162Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"\n    [INST] Generate \"socratic\" guidance as a \"Socratic Guiding Assistant\" to help the User debug their code. The generated guidance must help the User realize their bug. The guidance must not directly reveal the bug fix, and must be coherent with the conversation so far.\n    You are given the following metadata:\n    1. Problem Description and Test Cases (<problem>)\n    2. Student's buggy code (<bug_code>)\n    3. Bug Description (<bug_desc>)\n    4. Bug Fixes (<bug_fixes>)\n    5. Conversation (between User and Assistant) so far (<CONVERSATION>)[/INST]\n\n    <METADATA>\n    <problem>\nWrite a function `factorial(n:int) -> int` that computes the factorial n! of a natural number n, which is defined mathematically as:\n\n$0! = 1$\n$n! = n \\times (n - 1)!$\n\nAdditionally, if the input integer n is negative the function should return 0.\n\n## Example Cases:\n```\nfactorial(-1) => 0\nfactorial(0) => 1\nfactorial(1) => 1\nfactorial(2) => 2\nfactorial(3) => 6\nfactorial(4) => 24\nfactorial(5) => 120\n```\n</problem>\n<bug_code>\n1. def factorial(n):\n2.        if n < 0:\n3.                return 0\n4.        fact = 1\n5.        for i in range(n):\n6.                fact = fact * i\n7.        return fact\n</bug_code>\n<bug_desc>\nOn line 6, `fact` is multiplied with 0 in the first iteration of the for loop. Consequently, at every iteration fact stays equal with 0 instead of being updated to be equal with factorial of `(i + 1)`. Therefore, the function will return 0, irrespective of n\n</bug_desc>\n<bug_fixes>\nReplace `i` with `(i + 1)` in line 6.\nReplace `range(n)` with `range(1, n + 1)` in line 5.\n</bug_fixes>\n    </METADATA>\n\n    <CONVERSATION>\n    User: Hi! I implemented the factorial function but it doesnâ€™t work and I do not know why. Can you help?\nAssistant: \n","output_type":"stream"}]},{"cell_type":"code","source":"inputs","metadata":{"execution":{"iopub.status.busy":"2024-10-31T14:57:11.947393Z","iopub.execute_input":"2024-10-31T14:57:11.947796Z","iopub.status.idle":"2024-10-31T14:57:11.977993Z","shell.execute_reply.started":"2024-10-31T14:57:11.947757Z","shell.execute_reply":"2024-10-31T14:57:11.976970Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[    1, 28705,    13,  2287,   733, 16289, 28793, 26075,   345, 28713,\n         26184, 28739, 15988,   390,   264,   345, 28735, 26184,  2480,  3269,\n         21631, 28739,   298,  1316,   272,  1247, 10183,   652,  2696, 28723,\n           415,  7138, 15988,  1580,  1316,   272,  1247,  8168,   652, 10079,\n         28723,   415, 15988,  1580,   459,  5090, 14918,   272, 10079,  6293,\n         28725,   304,  1580,   347,  1001, 21891,   395,   272,  7114,   579,\n          2082, 28723,    13,  2287,   995,   460,  2078,   272,  2296, 11214,\n         28747,    13,   260, 28740, 28723, 24857, 10220,   304,  3735,   334,\n          2018,   325, 28789,   729, 11706, 12970,    13,   260, 28750, 28723,\n         15965, 28742, 28713, 10079,  1495,  2696,   325, 28789,  3718, 28730,\n          1409, 12970,    13,   260, 28770, 28723, 27431, 10220,   325, 28789,\n          3718, 28730,  5916, 12970,    13,   260, 28781, 28723, 27431, 23047,\n           274,   325, 28789,  3718, 28730,  7192,   274, 12970,    13,   260,\n         28782, 28723,  1325, 25422,   325, 24829,  1247,   304, 21631, 28731,\n           579,  2082,   325, 28789,  3185,  3491, 28735,  4866, 28767, 10908,\n         28748, 16289, 28793,    13,    13,  2287,   523,  1574, 28738,  1841,\n          5854, 28767,    13,  2287,   523,   729, 11706, 28767,    13,  5238,\n           264,   908,  1552, 18360,   505, 28732, 28711, 28747,   501, 28731,\n          3193,   716, 28832,   369,   623,  2907,   272,  6999,   505,   307,\n         28808,   302,   264,  4229,  1474,   307, 28725,   690,   349,  4057,\n          1610,  6981,  6789,   390, 28747,    13,    13, 28776, 28734, 28808,\n           327, 28705, 28740, 28776,    13, 28776, 28711, 28808,   327,   307,\n           414,  4593,   325, 28711,   387, 28705, 28740, 28731, 28808, 28776,\n            13,    13,  1991, 14586, 28725,   513,   272,  2787, 11584,   307,\n           349,  7087,   272,   908,  1023,   604, 28705, 28734, 28723,    13,\n            13,  1064, 16693,   334,  2018, 28747,    13, 13940, 28832,    13,\n         18360,   505,  6422, 28740, 28731,   953, 28705, 28734,    13, 18360,\n           505, 28732, 28734, 28731,   953, 28705, 28740,    13, 18360,   505,\n         28732, 28740, 28731,   953, 28705, 28740,    13, 18360,   505, 28732,\n         28750, 28731,   953, 28705, 28750,    13, 18360,   505, 28732, 28770,\n         28731,   953, 28705, 28784,    13, 18360,   505, 28732, 28781, 28731,\n           953, 28705, 28750, 28781,    13, 18360,   505, 28732, 28782, 28731,\n           953, 28705, 28740, 28750, 28734,    13, 13940, 28832,    13,   700,\n           729, 11706, 28767,    13, 28789,  3718, 28730,  1409, 28767,    13,\n         28740, 28723,   801,  6999,   505, 28732, 28711,  1329,    13, 28750,\n         28723,  5390,   513,   307,   523, 28705, 28734, 28747,    13, 28770,\n         28723,  1417, 28705,   604, 28705, 28734,    13, 28781, 28723,  5390,\n          1639,   327, 28705, 28740,    13, 28782, 28723,  5390,   354,   613,\n           297,  2819, 28732, 28711,  1329,    13, 28784, 28723,  1417, 28705,\n          1639,   327,  1639,   398,   613,    13, 28787, 28723,  5390,   604,\n          1639,    13,   700,  3718, 28730,  1409, 28767,    13, 28789,  3718,\n         28730,  5916, 28767,    13,  2486,  1407, 28705, 28784, 28725,  1552,\n         22313, 28832,   349,  6079,  3002,   395, 28705, 28734,   297,   272,\n           907, 23113,   302,   272,   354,  7870, 28723,  1325, 23321, 28725,\n           438,  1012, 23113,  1639, 22361,  6530,   395, 28705, 28734,  3519,\n           302,  1250,  8457,   298,   347,  6530,   395,  6999,   505,   302,\n          1552, 28732, 28710,   648, 28705, 28740, 28731,  9429,  8469, 28725,\n           272,   908,   622,   604, 28705, 28734, 28725,  4139,   411,  8524,\n           302,   307,    13,   700,  3718, 28730,  5916, 28767,    13, 28789,\n          3718, 28730,  7192,   274, 28767,    13, 21392,  1552, 28710, 28832,\n           395,  1552, 28732, 28710,   648, 28705, 28740, 28731, 28832,   297,\n          1407, 28705, 28784, 28723,    13, 21392,  1552,  6347, 28732, 28711,\n         28731, 28832,   395,  1552,  6347, 28732, 28740, 28725,   307,   648,\n         28705, 28740, 28731, 28832,   297,  1407, 28705, 28782, 28723,    13,\n           700,  3718, 28730,  7192,   274, 28767,    13,  2287,  1867,  1574,\n         28738,  1841,  5854, 28767,    13,    13,  2287,   523,  3185,  3491,\n         28735,  4866, 28767,    13,  2287,  1247, 28747, 15359, 28808,   315,\n         13492,   272,  6999,   505,   908,   562,   378,  2368, 28809, 28707,\n           771,   304,   315,   511,   459,   873,  2079, 28723,  2418,   368,\n          1316, 28804,    13,  7226, 11143, 28747, 28705]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n       device='cuda:0')}"},"metadata":{}}]},{"cell_type":"code","source":"response","metadata":{"execution":{"iopub.status.busy":"2024-10-31T14:58:06.957555Z","iopub.execute_input":"2024-10-31T14:58:06.957941Z","iopub.status.idle":"2024-10-31T14:58:06.965962Z","shell.execute_reply.started":"2024-10-31T14:58:06.957905Z","shell.execute_reply":"2024-10-31T14:58:06.965111Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"['<s> \\n    [INST] Generate \"socratic\" guidance as a \"Socratic Guiding Assistant\" to help the User debug their code. The generated guidance must help the User realize their bug. The guidance must not directly reveal the bug fix, and must be coherent with the conversation so far.\\n    You are given the following metadata:\\n    1. Problem Description and Test Cases (<problem>)\\n    2. Student\\'s buggy code (<bug_code>)\\n    3. Bug Description (<bug_desc>)\\n    4. Bug Fixes (<bug_fixes>)\\n    5. Conversation (between User and Assistant) so far (<CONVERSATION>)[/INST]\\n\\n    <METADATA>\\n    <problem>\\nWrite a function `factorial(n:int) -> int` that computes the factorial n! of a natural number n, which is defined mathematically as:\\n\\n$0! = 1$\\n$n! = n \\\\times (n - 1)!$\\n\\nAdditionally, if the input integer n is negative the function should return 0.\\n\\n## Example Cases:\\n```\\nfactorial(-1) => 0\\nfactorial(0) => 1\\nfactorial(1) => 1\\nfactorial(2) => 2\\nfactorial(3) => 6\\nfactorial(4) => 24\\nfactorial(5) => 120\\n```\\n</problem>\\n<bug_code>\\n1. def factorial(n):\\n2.        if n < 0:\\n3.                return 0\\n4.        fact = 1\\n5.        for i in range(n):\\n6.                fact = fact * i\\n7.        return fact\\n</bug_code>\\n<bug_desc>\\nOn line 6, `fact` is multiplied with 0 in the first iteration of the for loop. Consequently, at every iteration fact stays equal with 0 instead of being updated to be equal with factorial of `(i + 1)`. Therefore, the function will return 0, irrespective of n\\n</bug_desc>\\n<bug_fixes>\\nReplace `i` with `(i + 1)` in line 6.\\nReplace `range(n)` with `range(1, n + 1)` in line 5.\\n</bug_fixes>\\n    </METADATA>\\n\\n    <CONVERSATION>\\n    User: Hi! I implemented the factorial function but it doesnâ€™t work and I do not know why. Can you help?\\nAssistant: </s>']"},"metadata":{}}]}]}