{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-30T00:39:35.630457Z","iopub.execute_input":"2024-10-30T00:39:35.630946Z","iopub.status.idle":"2024-10-30T00:39:36.054668Z","shell.execute_reply.started":"2024-10-30T00:39:35.630895Z","shell.execute_reply":"2024-10-30T00:39:36.053042Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/train-data/good_outputs.json\n/kaggle/input/train-data/preference_data.json\n/kaggle/input/train-data/input_prompts.json\n/kaggle/input/train-data/problem_metadata.json\n/kaggle/input/test-data/good_outputs.json\n/kaggle/input/test-data/input_prompts.json\n/kaggle/input/test-data/problem_metadata.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The trl library is a full stack tool to fine-tune and align transformer language and diffusion models using methods such as Supervised Fine-tuning step (SFT), Reward Modeling (RM) and the Proximal Policy Optimization (PPO) as well as Direct Preference Optimization (DPO).","metadata":{}},{"cell_type":"code","source":"pip install trl","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:39:36.056777Z","iopub.execute_input":"2024-10-30T00:39:36.057309Z","iopub.status.idle":"2024-10-30T00:39:49.850614Z","shell.execute_reply.started":"2024-10-30T00:39:36.057263Z","shell.execute_reply":"2024-10-30T00:39:49.849031Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting trl\n  Downloading trl-0.11.4-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.4.0)\nRequirement already satisfied: transformers>=4.40.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.45.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl) (0.34.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (3.0.1)\nCollecting tyro>=0.5.11 (from trl)\n  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (0.25.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (4.66.4)\nRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.1)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl) (5.9.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.40.0->trl) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl) (2024.8.30)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\nDownloading trl-0.11.4-py3-none-any.whl (316 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nInstalling collected packages: shtab, tyro, trl\nSuccessfully installed shtab-1.7.1 trl-0.11.4 tyro-0.8.14\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:39:49.852361Z","iopub.execute_input":"2024-10-30T00:39:49.852687Z","iopub.status.idle":"2024-10-30T00:40:02.129066Z","shell.execute_reply.started":"2024-10-30T00:39:49.852654Z","shell.execute_reply":"2024-10-30T00:40:02.127809Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install -U bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:02.131401Z","iopub.execute_input":"2024-10-30T00:40:02.131744Z","iopub.status.idle":"2024-10-30T00:40:18.651641Z","shell.execute_reply.started":"2024-10-30T00:40:02.131708Z","shell.execute_reply":"2024-10-30T00:40:18.650447Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from typing import Optional\nimport argparse\nimport os\nimport sys\nimport json\nfrom itertools import combinations\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import TrainingArguments, Trainer, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom trl import PPOTrainer, PPOConfig, DPOTrainer, AutoModelForCausalLMWithValueHead\nfrom peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import Dataset as HFDataset\nfrom tqdm import tqdm\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:18.653102Z","iopub.execute_input":"2024-10-30T00:40:18.653427Z","iopub.status.idle":"2024-10-30T00:40:38.426183Z","shell.execute_reply.started":"2024-10-30T00:40:18.653392Z","shell.execute_reply":"2024-10-30T00:40:38.425353Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.427407Z","iopub.execute_input":"2024-10-30T00:40:38.427718Z","iopub.status.idle":"2024-10-30T00:40:38.457729Z","shell.execute_reply.started":"2024-10-30T00:40:38.427678Z","shell.execute_reply":"2024-10-30T00:40:38.456706Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n#     load_in_8bit=True,\n    load_in_4bit=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.459163Z","iopub.execute_input":"2024-10-30T00:40:38.459469Z","iopub.status.idle":"2024-10-30T00:40:38.486485Z","shell.execute_reply.started":"2024-10-30T00:40:38.459436Z","shell.execute_reply":"2024-10-30T00:40:38.485638Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"This configuration defines settings for LoRA (Low-Rank Adaptation), a parameter-efficient fine-tuning method that optimizes specific layers in a large model, using the LoraConfig class. Here’s what each parameter does:\n\n**target_modules**: This specifies which layers in the model will be fine-tuned. Here, it includes layers like \"q_proj\", \"k_proj\", \"v_proj\", and \"o_proj\" (often associated with self-attention operations) as well as \"gate_proj\", \"up_proj\", and \"down_proj\" (often related to feedforward network layers). By targeting only these layers, LoRA reduces the number of parameters being trained, making the process more efficient.\n\n**r**: This represents the rank of the low-rank decomposition in LoRA. A higher rank means more parameters will be used in the decomposition. Here, it’s set to 32, providing a balance between model expressivity and parameter efficiency.\n\n**lora_alpha**: This scaling factor (set to 16) controls the extent to which the LoRA adjustments are scaled. Higher values increase the impact of the LoRA updates on the model, potentially improving learning but also increasing the risk of overfitting.\n\n**lora_dropout**: Set to 0.05, this applies dropout to the LoRA layers. Adding dropout introduces regularization, reducing overfitting by randomly deactivating some parameters during training.\n\n**task_type**: This indicates the type of task the model is being fine-tuned for. Here, it’s set to \"CAUSAL_LM\", meaning causal language modeling, which is common for autoregressive tasks where the model predicts the next token in a sequence.\n\n**inference_mode**: Set to False, this means the configuration is intended for training rather than inference. If True, it would optimize for inference, using the trained LoRA layers without further updates.","metadata":{}},{"cell_type":"code","source":"peft_config = LoraConfig(\n    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    r=32,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    task_type=\"CAUSAL_LM\",\n    inference_mode=False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.487680Z","iopub.execute_input":"2024-10-30T00:40:38.488008Z","iopub.status.idle":"2024-10-30T00:40:38.497253Z","shell.execute_reply.started":"2024-10-30T00:40:38.487963Z","shell.execute_reply":"2024-10-30T00:40:38.496472Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def get_base_model(base_model_name: str, tokenizer: AutoTokenizer, test: bool):\n    base_model = AutoModelForCausalLM.from_pretrained(\n        base_model_name,\n        pad_token_id=tokenizer.pad_token_id,\n        quantization_config=None if test else bnb_config,\n        # Higher precision for non-quantized parameters helps training accuracy and doesn't hurt performance\n        # Lower precision at test time improves speed and only marginally hurts performance\n        torch_dtype=torch.float16 if test else torch.float32,\n        device_map={\"\": 0}\n    )\n    base_model.config.use_cache = False\n    base_model.config.pretraining_tp = 1\n    return base_model","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.498421Z","iopub.execute_input":"2024-10-30T00:40:38.498792Z","iopub.status.idle":"2024-10-30T00:40:38.511497Z","shell.execute_reply.started":"2024-10-30T00:40:38.498746Z","shell.execute_reply":"2024-10-30T00:40:38.510638Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def get_model(base_model_name: str, model_name: Optional[str], pt_model_name: Optional[str],\n              include_value_head: bool, test: bool, use_gradient_checkpointing: bool = True):\n    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n    model = get_base_model(base_model_name, tokenizer, test)\n    if test:\n        # TODO: recommended to merge from quantized model - https://huggingface.co/docs/trl/main/en/dpo_trainer#downsides-to-merging-qlora-before-dpo-approach-2\n        # If model was adapted on top of pre-trained model, load the pre-trained adapter first\n        if pt_model_name:\n            model = PeftModel.from_pretrained(model, pt_model_name).merge_and_unload()\n        model = PeftModel.from_pretrained(model, model_name).merge_and_unload()\n    else:\n        # The pre-trained model serves as the base for the peft model AND as the reference model for KL regularization\n        # The trl API can disable the adapters on the peft model to recover the reference model\n        # Thus the reference model needs to be merged and unloaded, and doing it while quantized to save memory\n        # Will cause \"UserWarning: Merge lora module to 8-bit linear may get different generations due to rounding errors.\"\n        if pt_model_name:\n            model = PeftModel.from_pretrained(model, pt_model_name).merge_and_unload()\n        # Create newly initialized LoRA adapters on top of base model\n        # Gradient checkpointing can be used to save memory at cost of some time\n        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=use_gradient_checkpointing)\n        model = get_peft_model(model, peft_config)\n        # Wrap model to add (newly initialized) value head for PPO training\n        if include_value_head:\n            model = AutoModelForCausalLMWithValueHead(model).to(device)\n            model.is_peft_model = True # Tells PPO trainer to disable adapters to recover reference model\n    return model, tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.514622Z","iopub.execute_input":"2024-10-30T00:40:38.514902Z","iopub.status.idle":"2024-10-30T00:40:38.523521Z","shell.execute_reply.started":"2024-10-30T00:40:38.514873Z","shell.execute_reply":"2024-10-30T00:40:38.522647Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"DATA","metadata":{}},{"cell_type":"code","source":"def construct_prompt(metadata, input_dialouge):\n    '''\n    return fixed prompt \n    '''\n\n    fix_prompt = '''\n    [INST] Generate \"socratic\" guidance as a \"Socratic Guiding Assistant\" to help the User debug their code. The generated guidance must help the User realize their bug. The guidance must not directly reveal the bug fix, and must be coherent with the conversation so far.\n    You are given the following metadata:\n    1. Problem Description and Test Cases (<problem>)\n    2. Student's buggy code (<bug_code>)\n    3. Bug Description (<bug_desc>)\n    4. Bug Fixes (<bug_fixes>)\n    5. Conversation (between User and Assistant) so far (<CONVERSATION>)[/INST]\n\n    <METADATA>\n    {}\n    </METADATA>\n\n    <CONVERSATION>\n    {}'''.format(metadata, input_dialouge)\n    \n    return fix_prompt","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.524599Z","iopub.execute_input":"2024-10-30T00:40:38.524898Z","iopub.status.idle":"2024-10-30T00:40:38.538782Z","shell.execute_reply.started":"2024-10-30T00:40:38.524854Z","shell.execute_reply":"2024-10-30T00:40:38.538033Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def construct_data(split_path: str, preference_data=False):\n    '''\n    create a list of dictionaries for SFT\n    '''\n    \n    \n    data_path = os.path.join('/kaggle/input/', split_path)\n    # input prompts \n    with open(os.path.join(data_path, 'input_prompts.json'), 'r') as infile:\n        input_dialouges_dict = json.load(infile)\n    # problem metadata\n    with open(os.path.join(data_path, 'problem_metadata.json'), 'r') as infile:\n        problem_metadata_dict = json.load(infile)\n    # good data \n    with open(os.path.join(data_path, 'good_outputs.json'), 'r') as infile:\n        good_outputs_dict = json.load(infile)\n    if preference_data:\n        # preference data\n        with open(os.path.join(data_path, 'preference_data.json'), 'r') as infile:\n            preference_data_dict = json.load(infile)\n    \n\n    all_data = []\n\n    for tr_file, metadata in tqdm(problem_metadata_dict.items(), total=len(problem_metadata_dict)):\n        input_dialouges = input_dialouges_dict[tr_file]\n        good_outputs_list = good_outputs_dict[tr_file]\n        for ctr, dialouge in enumerate(input_dialouges):\n            # construct prompt\n            fix_prompt = construct_prompt(metadata, dialouge)\n            if not preference_data:\n                \n                if split_path == 'testset':\n                    all_data.append({'prompt': fix_prompt, 'output': str(good_outputs_list[ctr])})\n                else:\n                    for good_output in good_outputs_list[ctr]:\n                        # append to all data\n                        all_data.append({'prompt': fix_prompt, 'output': good_output+'</CONVERSATION>'})\n            else:\n                preference_data_list = preference_data_dict[tr_file]\n                # create dataset for DPO training\n                for preference_tuples in preference_data_list[ctr]:\n                    # append to all data\n                    all_data.append({'prompt': fix_prompt, 'chosen': str(preference_tuples[0])+'</CONVERSATION>', 'rejected': str(preference_tuples[1])+'</CONVERSATION>'})\n                \n    \n    return all_data","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.539960Z","iopub.execute_input":"2024-10-30T00:40:38.540546Z","iopub.status.idle":"2024-10-30T00:40:38.552765Z","shell.execute_reply.started":"2024-10-30T00:40:38.540513Z","shell.execute_reply":"2024-10-30T00:40:38.551917Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class QGSFTDataset(Dataset):\n    '''\n    QG Dataset\n    '''\n    def __init__(self, data: list):\n        self.data = data\n    \n    def __getitem__(self, index: int):\n        return self.data[index]\n\n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.553763Z","iopub.execute_input":"2024-10-30T00:40:38.554069Z","iopub.status.idle":"2024-10-30T00:40:38.567653Z","shell.execute_reply.started":"2024-10-30T00:40:38.554033Z","shell.execute_reply":"2024-10-30T00:40:38.566791Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class QGSFTCollator:\n    def __init__(self, tokenizer, test: bool):\n        self.tokenizer = tokenizer\n        self.test = test\n\n    def __call__(self, batch):\n        all_prompts = [sample[\"prompt\"] for sample in batch]\n        prompts_tokenized = self.tokenizer(all_prompts, return_tensors=\"pt\", padding=True)\n        if self.test:\n            return {\n                \"input_ids\": prompts_tokenized.input_ids.to(device),\n                \"attention_mask\": prompts_tokenized.attention_mask.to(device),\n                \"meta_data\": batch\n            }\n\n        # TODO: might be worth debugging this\n        all_inputs = [sample[\"prompt\"] + sample[\"output\"] + self.tokenizer.eos_token for sample in batch]\n        inputs_tokenized = self.tokenizer(all_inputs, return_tensors=\"pt\", padding=True)\n        prompt_lens = prompts_tokenized.attention_mask.sum(dim=1)\n        labels = inputs_tokenized.input_ids.clone()\n        padding_mask = torch.arange(labels.shape[1]).repeat(labels.shape[0], 1) < prompt_lens.unsqueeze(1)\n        labels[padding_mask] = -100\n        labels = labels.masked_fill(inputs_tokenized.attention_mask == 0, -100)\n        return {\n            \"input_ids\": inputs_tokenized.input_ids,\n            \"attention_mask\": inputs_tokenized.attention_mask,\n            \"labels\": labels\n        }","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.568704Z","iopub.execute_input":"2024-10-30T00:40:38.569040Z","iopub.status.idle":"2024-10-30T00:40:38.579076Z","shell.execute_reply.started":"2024-10-30T00:40:38.569000Z","shell.execute_reply":"2024-10-30T00:40:38.578248Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class QGDPODataset(Dataset):\n    '''\n    QG Dataset\n    '''\n    def __init__(self, data: list):\n        self.data = data\n    \n    def __getitem__(self, index: int):\n        return self.data[index]\n\n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.580018Z","iopub.execute_input":"2024-10-30T00:40:38.580332Z","iopub.status.idle":"2024-10-30T00:40:38.589643Z","shell.execute_reply.started":"2024-10-30T00:40:38.580273Z","shell.execute_reply":"2024-10-30T00:40:38.588760Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"TRAINING","metadata":{}},{"cell_type":"code","source":"def get_training_args(model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb):\n    return TrainingArguments(\n        output_dir=model_name,\n        num_train_epochs=epochs,\n        learning_rate=lr,\n        weight_decay=wd,\n        max_grad_norm=max_grad_norm or None,\n        per_device_train_batch_size=batch_size,\n        gradient_accumulation_steps=grad_accum_steps,\n        per_device_eval_batch_size=batch_size * 2,\n        eval_accumulation_steps=4,\n        warmup_ratio=0.1,\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        save_total_limit=1,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"loss\",\n        greater_is_better=False,\n        remove_unused_columns=False,\n        logging_steps=1,               # Log training loss every batch\n        log_level='info',\n        report_to=\"wandb\" if wandb else \"none\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.590694Z","iopub.execute_input":"2024-10-30T00:40:38.591050Z","iopub.status.idle":"2024-10-30T00:40:38.603152Z","shell.execute_reply.started":"2024-10-30T00:40:38.591007Z","shell.execute_reply":"2024-10-30T00:40:38.602272Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def sft(base_model, model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb):\n#     assert args.model_name\n\n\n    # construct data\n    print('#### Constructing Data ####')\n    all_train_data = construct_data(split_path='train-data')\n    \n    print(\"#### Splitting Data ####\")\n    # split into train and val (80-20)\n    train_data, val_data = train_test_split(all_train_data, test_size=0.2, random_state=37)\n    \n    print('#### get model and tokenizer ####')\n    model, tokenizer = get_model(base_model, None, None, False, False)\n    \n    print(\"#### Run training configuration ####\")\n    trainer = Trainer(\n        model=model,\n        args=get_training_args(model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb),\n        train_dataset=QGSFTDataset(train_data),\n        eval_dataset=QGSFTDataset(val_data),\n        data_collator=QGSFTCollator(tokenizer, False)\n    )\n    \n    print(\"#### train model ####\")\n    trainer.train()\n    \n    print(\"#### save model ####\")\n    trainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.604313Z","iopub.execute_input":"2024-10-30T00:40:38.604668Z","iopub.status.idle":"2024-10-30T00:40:38.615187Z","shell.execute_reply.started":"2024-10-30T00:40:38.604624Z","shell.execute_reply":"2024-10-30T00:40:38.614331Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"SANITY CHECKS","metadata":{}},{"cell_type":"code","source":"# def check_max_token_len(base_model, split_type='train'):\n#     # load data \n#     if split_type == 'train':\n#         data = construct_data(split_path='train-data')\n#     elif split_type == 'testset':\n#         data = construct_data(split_path='test-data')\n\n#     tokenizer = AutoTokenizer.from_pretrained(base_model)\n#     tokenizer.pad_token = tokenizer.eos_token\n#     tokenizer.padding_side = \"right\"\n#     max_len_full_prompt = 0\n#     max_len_full_output = 0\n#     for sample in tqdm(data, total = len(data)):\n#         input_sample = sample['prompt'] + sample['output'] + tokenizer.eos_token\n#         tokenized = tokenizer.encode(input_sample)\n#         tokenize_output = tokenizer.encode(sample['output'])\n#         max_len_full_prompt = max(max_len_full_prompt, len(tokenized))\n#         max_len_full_output = max(max_len_full_output, len(tokenize_output))\n    \n#     print('#### Split Type: {} ####'.format(split_type))\n#     print(f\"Maximum tokenized length: {max_len_full_prompt}\")\n#     print(f\"Maximum tokenized length: {max_len_full_output}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.616264Z","iopub.execute_input":"2024-10-30T00:40:38.616524Z","iopub.status.idle":"2024-10-30T00:40:38.627510Z","shell.execute_reply.started":"2024-10-30T00:40:38.616495Z","shell.execute_reply":"2024-10-30T00:40:38.626620Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# def perform_sanity_checks(base_model):\n#     # check maximum tokenized length of the inputs \n#     check_max_token_len(base_model, split_type='train')\n#     check_max_token_len(base_model, split_type='testset')","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.628485Z","iopub.execute_input":"2024-10-30T00:40:38.628778Z","iopub.status.idle":"2024-10-30T00:40:38.640231Z","shell.execute_reply.started":"2024-10-30T00:40:38.628747Z","shell.execute_reply":"2024-10-30T00:40:38.639403Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"MAIN","metadata":{}},{"cell_type":"code","source":"# python finetune/sft_dpo.py --sft --base_model codellama/CodeLlama-7b-Instruct-hf --model_name codellama_sft_b2 --batch_size 2 --grad_accum_steps 32 --epochs 5","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.641393Z","iopub.execute_input":"2024-10-30T00:40:38.641737Z","iopub.status.idle":"2024-10-30T00:40:38.649454Z","shell.execute_reply.started":"2024-10-30T00:40:38.641684Z","shell.execute_reply":"2024-10-30T00:40:38.648664Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# base_model = \"codellama/CodeLlama-7b-Instruct-hf\"\nbase_model = \"microsoft/Phi-3.5-mini-instruct\"\n# model_name = \"codellama_sft_b2\"\nmodel_name = \"Phi-3.5_sft_b2\"\nbatch_size = 2\ngrad_accum_steps = 32\nepochs = 5\nmax_grad_norm = 1.0\nlr = 3e-5\nwd = 0.0\nwandb = \"wandb\"","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.650821Z","iopub.execute_input":"2024-10-30T00:40:38.651331Z","iopub.status.idle":"2024-10-30T00:40:38.660471Z","shell.execute_reply.started":"2024-10-30T00:40:38.651290Z","shell.execute_reply":"2024-10-30T00:40:38.659540Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.661667Z","iopub.execute_input":"2024-10-30T00:40:38.662046Z","iopub.status.idle":"2024-10-30T00:40:38.670294Z","shell.execute_reply.started":"2024-10-30T00:40:38.662003Z","shell.execute_reply":"2024-10-30T00:40:38.669446Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"sft(base_model, model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T00:40:38.671404Z","iopub.execute_input":"2024-10-30T00:40:38.671718Z","iopub.status.idle":"2024-10-30T12:31:43.151161Z","shell.execute_reply.started":"2024-10-30T00:40:38.671680Z","shell.execute_reply":"2024-10-30T12:31:43.150305Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"#### Constructing Data ####\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 134/134 [00:00<00:00, 29333.86it/s]","output_type":"stream"},{"name":"stdout","text":"#### Splitting Data ####\n#### get model and tokenizer ####\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c0e10ea05014be59afaed56b6ab0c78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eb943281aa243b989d26cc5f1751f00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d29dff2e9b94d05ad4b2934e0d5e9b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe1d17ce55f74dc7a71e98c56acf0ae2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cca79cb10c148fe8a54d42c0e59ad23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.45k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ba384cb441d49d1991567417cb052ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55d5e54406ec4770b178461e8947f309"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97e9384048fb44a6bef6bf9fe8f7301f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23010efc1dc24ce7bc3c933282f9b357"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a39877e10c494100a5039a20cb7674f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed4f85fe94f44a1b803497887078e8db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38d44a0c5e324010ac165449c1c3987b"}},"metadata":{}},{"name":"stdout","text":"#### Run training configuration ####\n#### train model ####\n","output_type":"stream"},{"name":"stderr","text":"***** Running training *****\n  Num examples = 1,529\n  Num Epochs = 5\n  Instantaneous batch size per device = 2\n  Total train batch size (w. parallel, distributed & accumulation) = 64\n  Gradient Accumulation steps = 32\n  Total optimization steps = 115\n  Number of trainable parameters = 17,825,792\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114040133331097, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"706baa408f9a4517ad2f19374aacedf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241030_004738-de1tnffc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tharooshavihidun/huggingface/runs/de1tnffc' target=\"_blank\">Phi-3.5_sft_b2</a></strong> to <a href='https://wandb.ai/tharooshavihidun/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tharooshavihidun/huggingface' target=\"_blank\">https://wandb.ai/tharooshavihidun/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tharooshavihidun/huggingface/runs/de1tnffc' target=\"_blank\">https://wandb.ai/tharooshavihidun/huggingface/runs/de1tnffc</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='115' max='115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [115/115 11:37:28, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.915900</td>\n      <td>1.924189</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.333300</td>\n      <td>1.304408</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.179300</td>\n      <td>1.166058</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.104200</td>\n      <td>1.140204</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.114400</td>\n      <td>1.135224</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples = 383\n  Batch size = 4\nSaving model checkpoint to Phi-3.5_sft_b2/checkpoint-23\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3.5-mini-instruct/snapshots/af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0/config.json\nModel config Phi3Config {\n  \"_name_or_path\": \"Phi-3.5-mini-instruct\",\n  \"architectures\": [\n    \"Phi3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"auto_map\": {\n    \"AutoConfig\": \"microsoft/Phi-3.5-mini-instruct--configuration_phi3.Phi3Config\",\n    \"AutoModelForCausalLM\": \"microsoft/Phi-3.5-mini-instruct--modeling_phi3.Phi3ForCausalLM\"\n  },\n  \"bos_token_id\": 1,\n  \"embd_pdrop\": 0.0,\n  \"eos_token_id\": 32000,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"model_type\": \"phi3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"original_max_position_embeddings\": 4096,\n  \"pad_token_id\": 32000,\n  \"resid_pdrop\": 0.0,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"long_factor\": [\n      1.0800000429153442,\n      1.1100000143051147,\n      1.1399999856948853,\n      1.340000033378601,\n      1.5899999141693115,\n      1.600000023841858,\n      1.6200000047683716,\n      2.620000123977661,\n      3.2300000190734863,\n      3.2300000190734863,\n      4.789999961853027,\n      7.400000095367432,\n      7.700000286102295,\n      9.09000015258789,\n      12.199999809265137,\n      17.670000076293945,\n      24.46000099182129,\n      28.57000160217285,\n      30.420001983642578,\n      30.840002059936523,\n      32.590003967285156,\n      32.93000411987305,\n      42.320003509521484,\n      44.96000289916992,\n      50.340003967285156,\n      50.45000457763672,\n      57.55000305175781,\n      57.93000411987305,\n      58.21000289916992,\n      60.1400032043457,\n      62.61000442504883,\n      62.62000274658203,\n      62.71000289916992,\n      63.1400032043457,\n      63.1400032043457,\n      63.77000427246094,\n      63.93000411987305,\n      63.96000289916992,\n      63.970001220703125,\n      64.02999877929688,\n      64.06999969482422,\n      64.08000183105469,\n      64.12000274658203,\n      64.41000366210938,\n      64.4800033569336,\n      64.51000213623047,\n      64.52999877929688,\n      64.83999633789062\n    ],\n    \"short_factor\": [\n      1.0,\n      1.0199999809265137,\n      1.0299999713897705,\n      1.0299999713897705,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0699999332427979,\n      1.0999999046325684,\n      1.1099998950958252,\n      1.1599998474121094,\n      1.1599998474121094,\n      1.1699998378753662,\n      1.2899998426437378,\n      1.339999794960022,\n      1.679999828338623,\n      1.7899998426437378,\n      1.8199998140335083,\n      1.8499997854232788,\n      1.8799997568130493,\n      1.9099997282028198,\n      1.9399996995925903,\n      1.9899996519088745,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0799996852874756,\n      2.0899996757507324,\n      2.189999580383301,\n      2.2199995517730713,\n      2.5899994373321533,\n      2.729999542236328,\n      2.749999523162842,\n      2.8399994373321533\n    ],\n    \"type\": \"longrope\"\n  },\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 262144,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32064\n}\n\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n\n***** Running Evaluation *****\n  Num examples = 383\n  Batch size = 4\nSaving model checkpoint to Phi-3.5_sft_b2/checkpoint-47\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3.5-mini-instruct/snapshots/af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0/config.json\nModel config Phi3Config {\n  \"_name_or_path\": \"Phi-3.5-mini-instruct\",\n  \"architectures\": [\n    \"Phi3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"auto_map\": {\n    \"AutoConfig\": \"microsoft/Phi-3.5-mini-instruct--configuration_phi3.Phi3Config\",\n    \"AutoModelForCausalLM\": \"microsoft/Phi-3.5-mini-instruct--modeling_phi3.Phi3ForCausalLM\"\n  },\n  \"bos_token_id\": 1,\n  \"embd_pdrop\": 0.0,\n  \"eos_token_id\": 32000,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"model_type\": \"phi3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"original_max_position_embeddings\": 4096,\n  \"pad_token_id\": 32000,\n  \"resid_pdrop\": 0.0,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"long_factor\": [\n      1.0800000429153442,\n      1.1100000143051147,\n      1.1399999856948853,\n      1.340000033378601,\n      1.5899999141693115,\n      1.600000023841858,\n      1.6200000047683716,\n      2.620000123977661,\n      3.2300000190734863,\n      3.2300000190734863,\n      4.789999961853027,\n      7.400000095367432,\n      7.700000286102295,\n      9.09000015258789,\n      12.199999809265137,\n      17.670000076293945,\n      24.46000099182129,\n      28.57000160217285,\n      30.420001983642578,\n      30.840002059936523,\n      32.590003967285156,\n      32.93000411987305,\n      42.320003509521484,\n      44.96000289916992,\n      50.340003967285156,\n      50.45000457763672,\n      57.55000305175781,\n      57.93000411987305,\n      58.21000289916992,\n      60.1400032043457,\n      62.61000442504883,\n      62.62000274658203,\n      62.71000289916992,\n      63.1400032043457,\n      63.1400032043457,\n      63.77000427246094,\n      63.93000411987305,\n      63.96000289916992,\n      63.970001220703125,\n      64.02999877929688,\n      64.06999969482422,\n      64.08000183105469,\n      64.12000274658203,\n      64.41000366210938,\n      64.4800033569336,\n      64.51000213623047,\n      64.52999877929688,\n      64.83999633789062\n    ],\n    \"short_factor\": [\n      1.0,\n      1.0199999809265137,\n      1.0299999713897705,\n      1.0299999713897705,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0699999332427979,\n      1.0999999046325684,\n      1.1099998950958252,\n      1.1599998474121094,\n      1.1599998474121094,\n      1.1699998378753662,\n      1.2899998426437378,\n      1.339999794960022,\n      1.679999828338623,\n      1.7899998426437378,\n      1.8199998140335083,\n      1.8499997854232788,\n      1.8799997568130493,\n      1.9099997282028198,\n      1.9399996995925903,\n      1.9899996519088745,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0799996852874756,\n      2.0899996757507324,\n      2.189999580383301,\n      2.2199995517730713,\n      2.5899994373321533,\n      2.729999542236328,\n      2.749999523162842,\n      2.8399994373321533\n    ],\n    \"type\": \"longrope\"\n  },\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 262144,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32064\n}\n\nDeleting older checkpoint [Phi-3.5_sft_b2/checkpoint-23] due to args.save_total_limit\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n\n***** Running Evaluation *****\n  Num examples = 383\n  Batch size = 4\nSaving model checkpoint to Phi-3.5_sft_b2/checkpoint-71\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3.5-mini-instruct/snapshots/af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0/config.json\nModel config Phi3Config {\n  \"_name_or_path\": \"Phi-3.5-mini-instruct\",\n  \"architectures\": [\n    \"Phi3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"auto_map\": {\n    \"AutoConfig\": \"microsoft/Phi-3.5-mini-instruct--configuration_phi3.Phi3Config\",\n    \"AutoModelForCausalLM\": \"microsoft/Phi-3.5-mini-instruct--modeling_phi3.Phi3ForCausalLM\"\n  },\n  \"bos_token_id\": 1,\n  \"embd_pdrop\": 0.0,\n  \"eos_token_id\": 32000,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"model_type\": \"phi3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"original_max_position_embeddings\": 4096,\n  \"pad_token_id\": 32000,\n  \"resid_pdrop\": 0.0,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"long_factor\": [\n      1.0800000429153442,\n      1.1100000143051147,\n      1.1399999856948853,\n      1.340000033378601,\n      1.5899999141693115,\n      1.600000023841858,\n      1.6200000047683716,\n      2.620000123977661,\n      3.2300000190734863,\n      3.2300000190734863,\n      4.789999961853027,\n      7.400000095367432,\n      7.700000286102295,\n      9.09000015258789,\n      12.199999809265137,\n      17.670000076293945,\n      24.46000099182129,\n      28.57000160217285,\n      30.420001983642578,\n      30.840002059936523,\n      32.590003967285156,\n      32.93000411987305,\n      42.320003509521484,\n      44.96000289916992,\n      50.340003967285156,\n      50.45000457763672,\n      57.55000305175781,\n      57.93000411987305,\n      58.21000289916992,\n      60.1400032043457,\n      62.61000442504883,\n      62.62000274658203,\n      62.71000289916992,\n      63.1400032043457,\n      63.1400032043457,\n      63.77000427246094,\n      63.93000411987305,\n      63.96000289916992,\n      63.970001220703125,\n      64.02999877929688,\n      64.06999969482422,\n      64.08000183105469,\n      64.12000274658203,\n      64.41000366210938,\n      64.4800033569336,\n      64.51000213623047,\n      64.52999877929688,\n      64.83999633789062\n    ],\n    \"short_factor\": [\n      1.0,\n      1.0199999809265137,\n      1.0299999713897705,\n      1.0299999713897705,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0699999332427979,\n      1.0999999046325684,\n      1.1099998950958252,\n      1.1599998474121094,\n      1.1599998474121094,\n      1.1699998378753662,\n      1.2899998426437378,\n      1.339999794960022,\n      1.679999828338623,\n      1.7899998426437378,\n      1.8199998140335083,\n      1.8499997854232788,\n      1.8799997568130493,\n      1.9099997282028198,\n      1.9399996995925903,\n      1.9899996519088745,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0799996852874756,\n      2.0899996757507324,\n      2.189999580383301,\n      2.2199995517730713,\n      2.5899994373321533,\n      2.729999542236328,\n      2.749999523162842,\n      2.8399994373321533\n    ],\n    \"type\": \"longrope\"\n  },\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 262144,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32064\n}\n\nDeleting older checkpoint [Phi-3.5_sft_b2/checkpoint-47] due to args.save_total_limit\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n\n***** Running Evaluation *****\n  Num examples = 383\n  Batch size = 4\nSaving model checkpoint to Phi-3.5_sft_b2/checkpoint-95\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3.5-mini-instruct/snapshots/af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0/config.json\nModel config Phi3Config {\n  \"_name_or_path\": \"Phi-3.5-mini-instruct\",\n  \"architectures\": [\n    \"Phi3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"auto_map\": {\n    \"AutoConfig\": \"microsoft/Phi-3.5-mini-instruct--configuration_phi3.Phi3Config\",\n    \"AutoModelForCausalLM\": \"microsoft/Phi-3.5-mini-instruct--modeling_phi3.Phi3ForCausalLM\"\n  },\n  \"bos_token_id\": 1,\n  \"embd_pdrop\": 0.0,\n  \"eos_token_id\": 32000,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"model_type\": \"phi3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"original_max_position_embeddings\": 4096,\n  \"pad_token_id\": 32000,\n  \"resid_pdrop\": 0.0,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"long_factor\": [\n      1.0800000429153442,\n      1.1100000143051147,\n      1.1399999856948853,\n      1.340000033378601,\n      1.5899999141693115,\n      1.600000023841858,\n      1.6200000047683716,\n      2.620000123977661,\n      3.2300000190734863,\n      3.2300000190734863,\n      4.789999961853027,\n      7.400000095367432,\n      7.700000286102295,\n      9.09000015258789,\n      12.199999809265137,\n      17.670000076293945,\n      24.46000099182129,\n      28.57000160217285,\n      30.420001983642578,\n      30.840002059936523,\n      32.590003967285156,\n      32.93000411987305,\n      42.320003509521484,\n      44.96000289916992,\n      50.340003967285156,\n      50.45000457763672,\n      57.55000305175781,\n      57.93000411987305,\n      58.21000289916992,\n      60.1400032043457,\n      62.61000442504883,\n      62.62000274658203,\n      62.71000289916992,\n      63.1400032043457,\n      63.1400032043457,\n      63.77000427246094,\n      63.93000411987305,\n      63.96000289916992,\n      63.970001220703125,\n      64.02999877929688,\n      64.06999969482422,\n      64.08000183105469,\n      64.12000274658203,\n      64.41000366210938,\n      64.4800033569336,\n      64.51000213623047,\n      64.52999877929688,\n      64.83999633789062\n    ],\n    \"short_factor\": [\n      1.0,\n      1.0199999809265137,\n      1.0299999713897705,\n      1.0299999713897705,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0699999332427979,\n      1.0999999046325684,\n      1.1099998950958252,\n      1.1599998474121094,\n      1.1599998474121094,\n      1.1699998378753662,\n      1.2899998426437378,\n      1.339999794960022,\n      1.679999828338623,\n      1.7899998426437378,\n      1.8199998140335083,\n      1.8499997854232788,\n      1.8799997568130493,\n      1.9099997282028198,\n      1.9399996995925903,\n      1.9899996519088745,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0799996852874756,\n      2.0899996757507324,\n      2.189999580383301,\n      2.2199995517730713,\n      2.5899994373321533,\n      2.729999542236328,\n      2.749999523162842,\n      2.8399994373321533\n    ],\n    \"type\": \"longrope\"\n  },\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 262144,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32064\n}\n\nDeleting older checkpoint [Phi-3.5_sft_b2/checkpoint-71] due to args.save_total_limit\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\nSaving model checkpoint to Phi-3.5_sft_b2/checkpoint-115\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3.5-mini-instruct/snapshots/af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0/config.json\nModel config Phi3Config {\n  \"_name_or_path\": \"Phi-3.5-mini-instruct\",\n  \"architectures\": [\n    \"Phi3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"auto_map\": {\n    \"AutoConfig\": \"microsoft/Phi-3.5-mini-instruct--configuration_phi3.Phi3Config\",\n    \"AutoModelForCausalLM\": \"microsoft/Phi-3.5-mini-instruct--modeling_phi3.Phi3ForCausalLM\"\n  },\n  \"bos_token_id\": 1,\n  \"embd_pdrop\": 0.0,\n  \"eos_token_id\": 32000,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"model_type\": \"phi3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"original_max_position_embeddings\": 4096,\n  \"pad_token_id\": 32000,\n  \"resid_pdrop\": 0.0,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"long_factor\": [\n      1.0800000429153442,\n      1.1100000143051147,\n      1.1399999856948853,\n      1.340000033378601,\n      1.5899999141693115,\n      1.600000023841858,\n      1.6200000047683716,\n      2.620000123977661,\n      3.2300000190734863,\n      3.2300000190734863,\n      4.789999961853027,\n      7.400000095367432,\n      7.700000286102295,\n      9.09000015258789,\n      12.199999809265137,\n      17.670000076293945,\n      24.46000099182129,\n      28.57000160217285,\n      30.420001983642578,\n      30.840002059936523,\n      32.590003967285156,\n      32.93000411987305,\n      42.320003509521484,\n      44.96000289916992,\n      50.340003967285156,\n      50.45000457763672,\n      57.55000305175781,\n      57.93000411987305,\n      58.21000289916992,\n      60.1400032043457,\n      62.61000442504883,\n      62.62000274658203,\n      62.71000289916992,\n      63.1400032043457,\n      63.1400032043457,\n      63.77000427246094,\n      63.93000411987305,\n      63.96000289916992,\n      63.970001220703125,\n      64.02999877929688,\n      64.06999969482422,\n      64.08000183105469,\n      64.12000274658203,\n      64.41000366210938,\n      64.4800033569336,\n      64.51000213623047,\n      64.52999877929688,\n      64.83999633789062\n    ],\n    \"short_factor\": [\n      1.0,\n      1.0199999809265137,\n      1.0299999713897705,\n      1.0299999713897705,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0699999332427979,\n      1.0999999046325684,\n      1.1099998950958252,\n      1.1599998474121094,\n      1.1599998474121094,\n      1.1699998378753662,\n      1.2899998426437378,\n      1.339999794960022,\n      1.679999828338623,\n      1.7899998426437378,\n      1.8199998140335083,\n      1.8499997854232788,\n      1.8799997568130493,\n      1.9099997282028198,\n      1.9399996995925903,\n      1.9899996519088745,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0799996852874756,\n      2.0899996757507324,\n      2.189999580383301,\n      2.2199995517730713,\n      2.5899994373321533,\n      2.729999542236328,\n      2.749999523162842,\n      2.8399994373321533\n    ],\n    \"type\": \"longrope\"\n  },\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 262144,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32064\n}\n\n\n***** Running Evaluation *****\n  Num examples = 383\n  Batch size = 4\nSaving model checkpoint to Phi-3.5_sft_b2/checkpoint-115\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3.5-mini-instruct/snapshots/af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0/config.json\nModel config Phi3Config {\n  \"_name_or_path\": \"Phi-3.5-mini-instruct\",\n  \"architectures\": [\n    \"Phi3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"auto_map\": {\n    \"AutoConfig\": \"microsoft/Phi-3.5-mini-instruct--configuration_phi3.Phi3Config\",\n    \"AutoModelForCausalLM\": \"microsoft/Phi-3.5-mini-instruct--modeling_phi3.Phi3ForCausalLM\"\n  },\n  \"bos_token_id\": 1,\n  \"embd_pdrop\": 0.0,\n  \"eos_token_id\": 32000,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"model_type\": \"phi3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"original_max_position_embeddings\": 4096,\n  \"pad_token_id\": 32000,\n  \"resid_pdrop\": 0.0,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"long_factor\": [\n      1.0800000429153442,\n      1.1100000143051147,\n      1.1399999856948853,\n      1.340000033378601,\n      1.5899999141693115,\n      1.600000023841858,\n      1.6200000047683716,\n      2.620000123977661,\n      3.2300000190734863,\n      3.2300000190734863,\n      4.789999961853027,\n      7.400000095367432,\n      7.700000286102295,\n      9.09000015258789,\n      12.199999809265137,\n      17.670000076293945,\n      24.46000099182129,\n      28.57000160217285,\n      30.420001983642578,\n      30.840002059936523,\n      32.590003967285156,\n      32.93000411987305,\n      42.320003509521484,\n      44.96000289916992,\n      50.340003967285156,\n      50.45000457763672,\n      57.55000305175781,\n      57.93000411987305,\n      58.21000289916992,\n      60.1400032043457,\n      62.61000442504883,\n      62.62000274658203,\n      62.71000289916992,\n      63.1400032043457,\n      63.1400032043457,\n      63.77000427246094,\n      63.93000411987305,\n      63.96000289916992,\n      63.970001220703125,\n      64.02999877929688,\n      64.06999969482422,\n      64.08000183105469,\n      64.12000274658203,\n      64.41000366210938,\n      64.4800033569336,\n      64.51000213623047,\n      64.52999877929688,\n      64.83999633789062\n    ],\n    \"short_factor\": [\n      1.0,\n      1.0199999809265137,\n      1.0299999713897705,\n      1.0299999713897705,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0699999332427979,\n      1.0999999046325684,\n      1.1099998950958252,\n      1.1599998474121094,\n      1.1599998474121094,\n      1.1699998378753662,\n      1.2899998426437378,\n      1.339999794960022,\n      1.679999828338623,\n      1.7899998426437378,\n      1.8199998140335083,\n      1.8499997854232788,\n      1.8799997568130493,\n      1.9099997282028198,\n      1.9399996995925903,\n      1.9899996519088745,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0799996852874756,\n      2.0899996757507324,\n      2.189999580383301,\n      2.2199995517730713,\n      2.5899994373321533,\n      2.729999542236328,\n      2.749999523162842,\n      2.8399994373321533\n    ],\n    \"type\": \"longrope\"\n  },\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 262144,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32064\n}\n\nDeleting older checkpoint [Phi-3.5_sft_b2/checkpoint-95] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from Phi-3.5_sft_b2/checkpoint-115 (score: 1.1352235078811646).\nSaving model checkpoint to Phi-3.5_sft_b2\n","output_type":"stream"},{"name":"stdout","text":"#### save model ####\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3.5-mini-instruct/snapshots/af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0/config.json\nModel config Phi3Config {\n  \"_name_or_path\": \"Phi-3.5-mini-instruct\",\n  \"architectures\": [\n    \"Phi3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"auto_map\": {\n    \"AutoConfig\": \"microsoft/Phi-3.5-mini-instruct--configuration_phi3.Phi3Config\",\n    \"AutoModelForCausalLM\": \"microsoft/Phi-3.5-mini-instruct--modeling_phi3.Phi3ForCausalLM\"\n  },\n  \"bos_token_id\": 1,\n  \"embd_pdrop\": 0.0,\n  \"eos_token_id\": 32000,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"model_type\": \"phi3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"original_max_position_embeddings\": 4096,\n  \"pad_token_id\": 32000,\n  \"resid_pdrop\": 0.0,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"long_factor\": [\n      1.0800000429153442,\n      1.1100000143051147,\n      1.1399999856948853,\n      1.340000033378601,\n      1.5899999141693115,\n      1.600000023841858,\n      1.6200000047683716,\n      2.620000123977661,\n      3.2300000190734863,\n      3.2300000190734863,\n      4.789999961853027,\n      7.400000095367432,\n      7.700000286102295,\n      9.09000015258789,\n      12.199999809265137,\n      17.670000076293945,\n      24.46000099182129,\n      28.57000160217285,\n      30.420001983642578,\n      30.840002059936523,\n      32.590003967285156,\n      32.93000411987305,\n      42.320003509521484,\n      44.96000289916992,\n      50.340003967285156,\n      50.45000457763672,\n      57.55000305175781,\n      57.93000411987305,\n      58.21000289916992,\n      60.1400032043457,\n      62.61000442504883,\n      62.62000274658203,\n      62.71000289916992,\n      63.1400032043457,\n      63.1400032043457,\n      63.77000427246094,\n      63.93000411987305,\n      63.96000289916992,\n      63.970001220703125,\n      64.02999877929688,\n      64.06999969482422,\n      64.08000183105469,\n      64.12000274658203,\n      64.41000366210938,\n      64.4800033569336,\n      64.51000213623047,\n      64.52999877929688,\n      64.83999633789062\n    ],\n    \"short_factor\": [\n      1.0,\n      1.0199999809265137,\n      1.0299999713897705,\n      1.0299999713897705,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0499999523162842,\n      1.0699999332427979,\n      1.0999999046325684,\n      1.1099998950958252,\n      1.1599998474121094,\n      1.1599998474121094,\n      1.1699998378753662,\n      1.2899998426437378,\n      1.339999794960022,\n      1.679999828338623,\n      1.7899998426437378,\n      1.8199998140335083,\n      1.8499997854232788,\n      1.8799997568130493,\n      1.9099997282028198,\n      1.9399996995925903,\n      1.9899996519088745,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0199997425079346,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0299997329711914,\n      2.0799996852874756,\n      2.0899996757507324,\n      2.189999580383301,\n      2.2199995517730713,\n      2.5899994373321533,\n      2.729999542236328,\n      2.749999523162842,\n      2.8399994373321533\n    ],\n    \"type\": \"longrope\"\n  },\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 262144,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32064\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\n\n# Create a zip file of the directory\nshutil.make_archive('/kaggle/working/checkpoint', 'zip', '/kaggle/working/Phi-3.5_sft_b2')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T12:34:15.229390Z","iopub.execute_input":"2024-10-30T12:34:15.229804Z","iopub.status.idle":"2024-10-30T12:34:31.321325Z","shell.execute_reply.started":"2024-10-30T12:34:15.229763Z","shell.execute_reply":"2024-10-30T12:34:31.320397Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/checkpoint.zip'"},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Display a download link\nFileLink(r'/kaggle/working/checkpoint.zip')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T12:39:44.586656Z","iopub.execute_input":"2024-10-30T12:39:44.587062Z","iopub.status.idle":"2024-10-30T12:39:44.599414Z","shell.execute_reply.started":"2024-10-30T12:39:44.587010Z","shell.execute_reply":"2024-10-30T12:39:44.598571Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/checkpoint.zip","text/html":"Path (<tt>/kaggle/working/checkpoint.zip</tt>) doesn't exist. It may still be in the process of being generated, or you may have the incorrect path."},"metadata":{}}]}]}