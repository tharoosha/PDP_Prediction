{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9752919,"sourceType":"datasetVersion","datasetId":5971378},{"sourceId":9753225,"sourceType":"datasetVersion","datasetId":5971596}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-31T12:41:08.102163Z","iopub.execute_input":"2024-10-31T12:41:08.102544Z","iopub.status.idle":"2024-10-31T12:41:08.492713Z","shell.execute_reply.started":"2024-10-31T12:41:08.102499Z","shell.execute_reply":"2024-10-31T12:41:08.491774Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/train-data/good_outputs.json\n/kaggle/input/train-data/preference_data.json\n/kaggle/input/train-data/input_prompts.json\n/kaggle/input/train-data/problem_metadata.json\n/kaggle/input/test-data/good_outputs.json\n/kaggle/input/test-data/input_prompts.json\n/kaggle/input/test-data/problem_metadata.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The trl library is a full stack tool to fine-tune and align transformer language and diffusion models using methods such as Supervised Fine-tuning step (SFT), Reward Modeling (RM) and the Proximal Policy Optimization (PPO) as well as Direct Preference Optimization (DPO).","metadata":{}},{"cell_type":"code","source":"# pip install trl","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:41:08.494720Z","iopub.execute_input":"2024-10-31T12:41:08.495353Z","iopub.status.idle":"2024-10-31T12:41:08.499399Z","shell.execute_reply.started":"2024-10-31T12:41:08.495303Z","shell.execute_reply":"2024-10-31T12:41:08.498446Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:41:08.500631Z","iopub.execute_input":"2024-10-31T12:41:08.500990Z","iopub.status.idle":"2024-10-31T12:41:08.509702Z","shell.execute_reply.started":"2024-10-31T12:41:08.500947Z","shell.execute_reply":"2024-10-31T12:41:08.508885Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# pip install -U bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:41:08.511605Z","iopub.execute_input":"2024-10-31T12:41:08.511937Z","iopub.status.idle":"2024-10-31T12:41:08.520375Z","shell.execute_reply.started":"2024-10-31T12:41:08.511905Z","shell.execute_reply":"2024-10-31T12:41:08.519560Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:03:38.233761Z","iopub.execute_input":"2024-10-31T15:03:38.234525Z","iopub.status.idle":"2024-10-31T15:03:38.529363Z","shell.execute_reply.started":"2024-10-31T15:03:38.234478Z","shell.execute_reply":"2024-10-31T15:03:38.528408Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama-3.1-8B-bnb-4bit on Math Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:41:09.300097Z","iopub.execute_input":"2024-10-31T12:41:09.300435Z","iopub.status.idle":"2024-10-31T12:41:16.514689Z","shell.execute_reply.started":"2024-10-31T12:41:09.300397Z","shell.execute_reply":"2024-10-31T12:41:16.513729Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtharooshavihidun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111294735555551, max=1.0)â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b9334a541214e8aafefe8aadb568357"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241031_124112-f2sfivpe</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tharooshavihidun/Fine-tune%20Llama-3.1-8B-bnb-4bit%20on%20Math%20Dataset/runs/f2sfivpe' target=\"_blank\">somber-howl-10</a></strong> to <a href='https://wandb.ai/tharooshavihidun/Fine-tune%20Llama-3.1-8B-bnb-4bit%20on%20Math%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tharooshavihidun/Fine-tune%20Llama-3.1-8B-bnb-4bit%20on%20Math%20Dataset' target=\"_blank\">https://wandb.ai/tharooshavihidun/Fine-tune%20Llama-3.1-8B-bnb-4bit%20on%20Math%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tharooshavihidun/Fine-tune%20Llama-3.1-8B-bnb-4bit%20on%20Math%20Dataset/runs/f2sfivpe' target=\"_blank\">https://wandb.ai/tharooshavihidun/Fine-tune%20Llama-3.1-8B-bnb-4bit%20on%20Math%20Dataset/runs/f2sfivpe</a>"},"metadata":{}}]},{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:41:16.516010Z","iopub.execute_input":"2024-10-31T12:41:16.516330Z","iopub.status.idle":"2024-10-31T12:44:41.852236Z","shell.execute_reply.started":"2024-10-31T12:41:16.516297Z","shell.execute_reply":"2024-10-31T12:44:41.851282Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n    \"unsloth/llama-2-7b-bnb-4bit\",\n    \"unsloth/llama-2-13b-bnb-4bit\",\n    \"unsloth/codellama-34b-bnb-4bit\",\n    \"unsloth/tinyllama-bnb-4bit\",\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/mistral-7b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:44:41.853951Z","iopub.execute_input":"2024-10-31T12:44:41.854759Z","iopub.status.idle":"2024-10-31T12:46:51.989804Z","shell.execute_reply.started":"2024-10-31T12:44:41.854709Z","shell.execute_reply":"2024-10-31T12:46:51.988871Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n==((====))==  Unsloth 2024.10.7: Fast Mistral patching. Transformers = 4.44.2.\n   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.888 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.5.1+cu121. CUDA = 6.0. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58baff44f8fa4a4b94db2f51e845b923"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/155 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03dee35527db455797a517625955e6cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdc40359aa404fe39e0ca847d305e2ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d8855daea584adeb2702e91186bf070"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0ee2e2312234861bf6eee2db631b72d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60bd23cdddce4f36919d00fac84bbf44"}},"metadata":{}},{"name":"stderr","text":"Unsloth: We fixed a gradient accumulation bug, but it seems like you don't have the latest transformers version!\nPlease update transformers, TRL and unsloth via:\n`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n","output_type":"stream"}]},{"cell_type":"code","source":"# pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:51.991058Z","iopub.execute_input":"2024-10-31T12:46:51.991719Z","iopub.status.idle":"2024-10-31T12:46:51.996810Z","shell.execute_reply.started":"2024-10-31T12:46:51.991681Z","shell.execute_reply":"2024-10-31T12:46:51.995835Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# !pip install --no-deps xformers trl peft accelerate bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:52.000453Z","iopub.execute_input":"2024-10-31T12:46:52.000808Z","iopub.status.idle":"2024-10-31T12:46:52.009710Z","shell.execute_reply.started":"2024-10-31T12:46:52.000777Z","shell.execute_reply":"2024-10-31T12:46:52.008835Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# from typing import Optional\n# # import argparse\n# import os\n# import sys\n# import json\n# # from itertools import combinations\n# # import pandas as pd\n# # import numpy as np\n# from sklearn.model_selection import train_test_split\n# # import torch\n# from torch.utils.data import DataLoader, Dataset\n# # from transformers import TrainingArguments, Trainer, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n# # from trl import PPOTrainer, PPOConfig, DPOTrainer, AutoModelForCausalLMWithValueHead\n# # from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n# from datasets import Dataset as HFDataset\n# from tqdm import tqdm\n# # import wandb","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:52.010825Z","iopub.execute_input":"2024-10-31T12:46:52.011090Z","iopub.status.idle":"2024-10-31T12:46:52.020016Z","shell.execute_reply.started":"2024-10-31T12:46:52.011060Z","shell.execute_reply":"2024-10-31T12:46:52.019159Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# bnb_config = BitsAndBytesConfig(\n# #     load_in_8bit=True,\n#     load_in_4bit=True\n# )","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:52.021111Z","iopub.execute_input":"2024-10-31T12:46:52.021775Z","iopub.status.idle":"2024-10-31T12:46:52.030433Z","shell.execute_reply.started":"2024-10-31T12:46:52.021741Z","shell.execute_reply":"2024-10-31T12:46:52.029559Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"This configuration defines settings for LoRA (Low-Rank Adaptation), a parameter-efficient fine-tuning method that optimizes specific layers in a large model, using the LoraConfig class. Hereâ€™s what each parameter does:\n\n**target_modules**: This specifies which layers in the model will be fine-tuned. Here, it includes layers like \"q_proj\", \"k_proj\", \"v_proj\", and \"o_proj\" (often associated with self-attention operations) as well as \"gate_proj\", \"up_proj\", and \"down_proj\" (often related to feedforward network layers). By targeting only these layers, LoRA reduces the number of parameters being trained, making the process more efficient.\n\n**r**: This represents the rank of the low-rank decomposition in LoRA. A higher rank means more parameters will be used in the decomposition. Here, itâ€™s set to 32, providing a balance between model expressivity and parameter efficiency.\n\n**lora_alpha**: This scaling factor (set to 16) controls the extent to which the LoRA adjustments are scaled. Higher values increase the impact of the LoRA updates on the model, potentially improving learning but also increasing the risk of overfitting.\n\n**lora_dropout**: Set to 0.05, this applies dropout to the LoRA layers. Adding dropout introduces regularization, reducing overfitting by randomly deactivating some parameters during training.\n\n**task_type**: This indicates the type of task the model is being fine-tuned for. Here, itâ€™s set to \"CAUSAL_LM\", meaning causal language modeling, which is common for autoregressive tasks where the model predicts the next token in a sequence.\n\n**inference_mode**: Set to False, this means the configuration is intended for training rather than inference. If True, it would optimize for inference, using the trained LoRA layers without further updates.","metadata":{}},{"cell_type":"code","source":"# peft_config = LoraConfig(\n#     # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n#     r=32,\n#     lora_alpha=16,\n#     lora_dropout=0.05,\n#     task_type=\"CAUSAL_LM\",\n#     inference_mode=False,\n# )","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:52.031554Z","iopub.execute_input":"2024-10-31T12:46:52.032123Z","iopub.status.idle":"2024-10-31T12:46:52.040125Z","shell.execute_reply.started":"2024-10-31T12:46:52.032081Z","shell.execute_reply":"2024-10-31T12:46:52.039387Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# def get_base_model(base_model_name: str, tokenizer: AutoTokenizer, test: bool):\n#     base_model = AutoModelForCausalLM.from_pretrained(\n#         base_model_name,\n#         pad_token_id=tokenizer.pad_token_id,\n#         quantization_config=None if test else bnb_config,\n#         # Higher precision for non-quantized parameters helps training accuracy and doesn't hurt performance\n#         # Lower precision at test time improves speed and only marginally hurts performance\n#         torch_dtype=torch.float16 if test else torch.float32,\n#         device_map={\"\": 0}\n#     )\n#     base_model.config.use_cache = False\n#     base_model.config.pretraining_tp = 1\n#     return base_model","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:52.041901Z","iopub.execute_input":"2024-10-31T12:46:52.042669Z","iopub.status.idle":"2024-10-31T12:46:52.052912Z","shell.execute_reply.started":"2024-10-31T12:46:52.042635Z","shell.execute_reply":"2024-10-31T12:46:52.052065Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# def get_model(base_model_name: str, model_name: Optional[str], pt_model_name: Optional[str],\n#               include_value_head: bool, test: bool, use_gradient_checkpointing: bool = True):\n#     tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n#     tokenizer.pad_token = tokenizer.eos_token\n#     tokenizer.padding_side = \"right\"\n#     model = get_base_model(base_model_name, tokenizer, test)\n#     if test:\n#         # TODO: recommended to merge from quantized model - https://huggingface.co/docs/trl/main/en/dpo_trainer#downsides-to-merging-qlora-before-dpo-approach-2\n#         # If model was adapted on top of pre-trained model, load the pre-trained adapter first\n#         if pt_model_name:\n#             model = PeftModel.from_pretrained(model, pt_model_name).merge_and_unload()\n#         model = PeftModel.from_pretrained(model, model_name).merge_and_unload()\n#     else:\n#         # The pre-trained model serves as the base for the peft model AND as the reference model for KL regularization\n#         # The trl API can disable the adapters on the peft model to recover the reference model\n#         # Thus the reference model needs to be merged and unloaded, and doing it while quantized to save memory\n#         # Will cause \"UserWarning: Merge lora module to 8-bit linear may get different generations due to rounding errors.\"\n#         if pt_model_name:\n#             model = PeftModel.from_pretrained(model, pt_model_name).merge_and_unload()\n#         # Create newly initialized LoRA adapters on top of base model\n#         # Gradient checkpointing can be used to save memory at cost of some time\n#         model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=use_gradient_checkpointing)\n#         model = get_peft_model(model, peft_config)\n#         # Wrap model to add (newly initialized) value head for PPO training\n#         if include_value_head:\n#             model = AutoModelForCausalLMWithValueHead(model).to(device)\n#             model.is_peft_model = True # Tells PPO trainer to disable adapters to recover reference model\n#     return model, tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:52.054261Z","iopub.execute_input":"2024-10-31T12:46:52.054585Z","iopub.status.idle":"2024-10-31T12:46:52.062948Z","shell.execute_reply.started":"2024-10-31T12:46:52.054527Z","shell.execute_reply":"2024-10-31T12:46:52.062152Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"DATA","metadata":{}},{"cell_type":"code","source":"prompt_style = \"\"\"\n    [INST] Generate \"socratic\" guidance as a \"Socratic Guiding Assistant\" to help the User debug their code. The generated guidance must help the User realize their bug. The guidance must not directly reveal the bug fix, and must be coherent with the conversation so far.\n    You are given the following metadata:\n    1. Problem Description and Test Cases (<problem>)\n    2. Student's buggy code (<bug_code>)\n    3. Bug Description (<bug_desc>)\n    4. Bug Fixes (<bug_fixes>)\n    5. Conversation (between User and Assistant) so far (<CONVERSATION>)[/INST]\n\n    <METADATA>\n    {}\n    </METADATA>\n\n    <CONVERSATION>\n    {}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:52.064010Z","iopub.execute_input":"2024-10-31T12:46:52.064393Z","iopub.status.idle":"2024-10-31T12:46:52.075679Z","shell.execute_reply.started":"2024-10-31T12:46:52.064361Z","shell.execute_reply":"2024-10-31T12:46:52.074889Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# def construct_prompt(metadata, input_dialouge):\n#     '''\n#     return fixed prompt \n#     '''\n\n#     fix_prompt = '''\n#     [INST] Generate \"socratic\" guidance as a \"Socratic Guiding Assistant\" to help the User debug their code. The generated guidance must help the User realize their bug. The guidance must not directly reveal the bug fix, and must be coherent with the conversation so far.\n#     You are given the following metadata:\n#     1. Problem Description and Test Cases (<problem>)\n#     2. Student's buggy code (<bug_code>)\n#     3. Bug Description (<bug_desc>)\n#     4. Bug Fixes (<bug_fixes>)\n#     5. Conversation (between User and Assistant) so far (<CONVERSATION>)[/INST]\n\n#     <METADATA>\n#     {}\n#     </METADATA>\n\n#     <CONVERSATION>\n#     {}'''.format(metadata, input_dialouge)\n    \n#     return fix_prompt","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:46:52.076674Z","iopub.execute_input":"2024-10-31T12:46:52.076922Z","iopub.status.idle":"2024-10-31T12:46:52.085497Z","shell.execute_reply.started":"2024-10-31T12:46:52.076893Z","shell.execute_reply":"2024-10-31T12:46:52.084723Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# input_dialouges_dict = load_dataset(\"json\", data_files=\"/kaggle/input/train-data/input_prompts.json\")\n# problem_metadata_dict = load_dataset(\"json\", data_files=\"/kaggle/input/train-data/problem_metadata.json\")\n# good_outputs_dict = load_dataset(\"json\", data_files=\"/kaggle/input/train-data/good_outputs.json\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:08:45.284387Z","iopub.execute_input":"2024-10-31T15:08:45.284809Z","iopub.status.idle":"2024-10-31T15:08:45.290138Z","shell.execute_reply.started":"2024-10-31T15:08:45.284770Z","shell.execute_reply":"2024-10-31T15:08:45.289136Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# len(problem_metadata_dict)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:08:52.060848Z","iopub.execute_input":"2024-10-31T15:08:52.061226Z","iopub.status.idle":"2024-10-31T15:08:52.066317Z","shell.execute_reply.started":"2024-10-31T15:08:52.061190Z","shell.execute_reply":"2024-10-31T15:08:52.065369Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"import json\ndata_path = os.path.join('/kaggle/input/train-data/')\n\n# input prompts \nwith open(os.path.join(data_path, 'input_prompts.json'), 'r') as infile:\n    input_dialouges_dict = json.load(infile)\n\n# problem metadata\nwith open(os.path.join(data_path, 'problem_metadata.json'), 'r') as infile:\n    problem_metadata_dict = json.load(infile)\n\n# good data \nwith open(os.path.join(data_path, 'good_outputs.json'), 'r') as infile:\n    good_outputs_dict = json.load(infile)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:14:11.828393Z","iopub.execute_input":"2024-10-31T13:14:11.828786Z","iopub.status.idle":"2024-10-31T13:14:11.851314Z","shell.execute_reply.started":"2024-10-31T13:14:11.828748Z","shell.execute_reply":"2024-10-31T13:14:11.850574Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:14:14.428074Z","iopub.execute_input":"2024-10-31T13:14:14.428600Z","iopub.status.idle":"2024-10-31T13:14:14.434597Z","shell.execute_reply.started":"2024-10-31T13:14:14.428546Z","shell.execute_reply":"2024-10-31T13:14:14.433611Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ntexts = []\nfor tr_file, metadata in tqdm(problem_metadata_dict.items(), total=len(problem_metadata_dict)):\n    input_dialouges = input_dialouges_dict[tr_file]\n\n    for ctr, dialouge in enumerate(input_dialouges):\n        text = prompt_style.format(metadata, dialouge) + EOS_TOKEN\n        texts.append(text)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:14:15.629585Z","iopub.execute_input":"2024-10-31T13:14:15.630522Z","iopub.status.idle":"2024-10-31T13:14:15.647510Z","shell.execute_reply.started":"2024-10-31T13:14:15.630459Z","shell.execute_reply":"2024-10-31T13:14:15.646250Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 134/134 [00:00<00:00, 27355.04it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def formatting_prompts_func():\n    texts = []\n    for tr_file, metadata in tqdm(problem_metadata_dict.items(), total=len(problem_metadata_dict)):\n        input_dialouges = input_dialouges_dict[tr_file]\n\n        for ctr, dialouge in enumerate(input_dialouges):\n            text = prompt_style.format(metadata, dialouge) + EOS_TOKEN\n            texts.append(text)\n    return {\"text\": texts}\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:14:17.508695Z","iopub.execute_input":"2024-10-31T13:14:17.509070Z","iopub.status.idle":"2024-10-31T13:14:17.517353Z","shell.execute_reply.started":"2024-10-31T13:14:17.509033Z","shell.execute_reply":"2024-10-31T13:14:17.516407Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"dataset = formatting_prompts_func()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:14:18.988310Z","iopub.execute_input":"2024-10-31T13:14:18.988701Z","iopub.status.idle":"2024-10-31T13:14:19.001071Z","shell.execute_reply.started":"2024-10-31T13:14:18.988663Z","shell.execute_reply":"2024-10-31T13:14:19.000160Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 134/134 [00:00<00:00, 44606.09it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.DataFrame(dataset[\"text\"], columns=[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:14:21.848152Z","iopub.execute_input":"2024-10-31T13:14:21.848550Z","iopub.status.idle":"2024-10-31T13:14:21.854400Z","shell.execute_reply.started":"2024-10-31T13:14:21.848494Z","shell.execute_reply":"2024-10-31T13:14:21.853579Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Save DataFrame to CSV format\ncsv_path = \"/kaggle/working/dataset.csv\"\ndf.to_csv(csv_path, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:14:29.788163Z","iopub.execute_input":"2024-10-31T13:14:29.788794Z","iopub.status.idle":"2024-10-31T13:14:29.891847Z","shell.execute_reply.started":"2024-10-31T13:14:29.788752Z","shell.execute_reply":"2024-10-31T13:14:29.890869Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:14:26.428300Z","iopub.execute_input":"2024-10-31T13:14:26.429125Z","iopub.status.idle":"2024-10-31T13:14:26.440442Z","shell.execute_reply.started":"2024-10-31T13:14:26.429069Z","shell.execute_reply":"2024-10-31T13:14:26.439563Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"                                                text\n0  \\n    [INST] Generate \"socratic\" guidance as a...\n1  \\n    [INST] Generate \"socratic\" guidance as a...\n2  \\n    [INST] Generate \"socratic\" guidance as a...\n3  \\n    [INST] Generate \"socratic\" guidance as a...\n4  \\n    [INST] Generate \"socratic\" guidance as a...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\n    [INST] Generate \"socratic\" guidance as a...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\\n    [INST] Generate \"socratic\" guidance as a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\\n    [INST] Generate \"socratic\" guidance as a...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\\n    [INST] Generate \"socratic\" guidance as a...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\n    [INST] Generate \"socratic\" guidance as a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(dataset[\"text\"][10])  # Display the first formatted entry","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:52:20.427644Z","iopub.execute_input":"2024-10-31T12:52:20.428012Z","iopub.status.idle":"2024-10-31T12:52:20.434241Z","shell.execute_reply.started":"2024-10-31T12:52:20.427979Z","shell.execute_reply":"2024-10-31T12:52:20.433316Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"\n    [INST] Generate \"socratic\" guidance as a \"Socratic Guiding Assistant\" to help the User debug their code. The generated guidance must help the User realize their bug. The guidance must not directly reveal the bug fix, and must be coherent with the conversation so far.\n    You are given the following metadata:\n    1. Problem Description and Test Cases (<problem>)\n    2. Student's buggy code (<bug_code>)\n    3. Bug Description (<bug_desc>)\n    4. Bug Fixes (<bug_fixes>)\n    5. Conversation (between User and Assistant) so far (<CONVERSATION>)[/INST]\n\n    <METADATA>\n    <problem>\nCreate a function `fibonacci(n:int)` that takes in a parameter `n`, an integer representing the number of terms in the Fibonacci sequence to generate, and returns:\n-  'OOPS', if `n` is less than or equal to 0 \n- `1`  if `n` is equal to `1`\n- `1` if `n` is equal to `2`\n- Otherwise, return the nth term of the Fibonacci sequence. A Fibonacci sequence is a sequence in which every number is the sum of the previous two numbers in the sequence. Example Fibonacci sequence: 1, 1, 2, 3, 5, 8, 13, 21, 34, 55 ....\n## Example Cases:\n```\nfibonacci(-3) => 'OOPS'\nfibonacci(0) => 'OOPS'\nfibonacci(1) =>1\nfibonacci(2) => 1\nfibonacci(5) => 5\nfibonacci(10) => 55\n```\n</problem>\n<bug_code>\n1. def fibonacci(n):\n2.    if n <= 0:\n3.        return \"OOPS\"\n4.    elif n == 1 or n == 2:\n5.        return 1\n6.    else:\n7.        return fibonacci(n) + fibonacci(n-1)\n</bug_code>\n<bug_desc>\n`fibonacci(n)` calls `fibonacci(n)` on line 7 with the same argument `n` which leads to infinite recursion and consequently a runtime error is thrown.\n</bug_desc>\n<bug_fixes>\nReplace `fibonacci(n) + fibonacci(n-1)` with `fibonacci(n-1) + fibonacci(n-2)` on line 7.\n</bug_fixes>\n    </METADATA>\n\n    <CONVERSATION>\n    User: Hi! My code returns a runtime error about infinite recursion, whenever I give it an input number greater than 2. Can you help?\nAssistant: </s>\n","output_type":"stream"}]},{"cell_type":"code","source":"# from datasets import Dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"csv\", data_files=\"/kaggle/input/socraticq-t/SocraticQ dataset (1).csv\", split = \"train\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:17:25.551792Z","iopub.execute_input":"2024-10-31T13:17:25.552167Z","iopub.status.idle":"2024-10-31T13:17:25.847482Z","shell.execute_reply.started":"2024-10-31T13:17:25.552133Z","shell.execute_reply":"2024-10-31T13:17:25.846526Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset.column_names)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:17:28.852880Z","iopub.execute_input":"2024-10-31T13:17:28.853718Z","iopub.status.idle":"2024-10-31T13:17:28.859674Z","shell.execute_reply.started":"2024-10-31T13:17:28.853676Z","shell.execute_reply":"2024-10-31T13:17:28.858704Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"['text']\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset['text'][0]","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:18:41.228672Z","iopub.execute_input":"2024-10-31T13:18:41.229550Z","iopub.status.idle":"2024-10-31T13:18:41.240309Z","shell.execute_reply.started":"2024-10-31T13:18:41.229489Z","shell.execute_reply":"2024-10-31T13:18:41.239388Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"'\\n    [INST] Generate \"socratic\" guidance as a \"Socratic Guiding Assistant\" to help the User debug their code. The generated guidance must help the User realize their bug. The guidance must not directly reveal the bug fix, and must be coherent with the conversation so far.\\n    You are given the following metadata:\\n    1. Problem Description and Test Cases (<problem>)\\n    2. Student\\'s buggy code (<bug_code>)\\n    3. Bug Description (<bug_desc>)\\n    4. Bug Fixes (<bug_fixes>)\\n    5. Conversation (between User and Assistant) so far (<CONVERSATION>)[/INST]\\n\\n    <METADATA>\\n    <problem>\\nThe CS110Z course director, unfortunately, was tricked into purchasing a Disney Vacation Club timeshare. The good news about DVC is that it lets you reserve a room at any Disney Resort for one week! The downside, however, is that members have to pay an annual \"maintenance fee\" so that the mouse can keep the property looking good (EVEN when Disney World was closed due to COVID-19 . . . yay).\\n\\nThis year, the maintenance was $623.00. If that isn\\'t bad enough, your course director discovered that maintenance fees aren\\'t fixed! On the contrary, it accues each year at a rate of approximately 1.5%.\\n\\nWrite a Python function called `get_years_until(target_value: float) -> int` that takes a target value as a parameter, and returns the number of years (assuming a fixed interest rate) before the maintenance fee exceeds this value.\\n\\n## Example Cases:\\n```\\nget_years_until(624) => 1\\nget_years_until(1000) => 32\\n```\\n\\n</problem>\\n<bug_code>\\n1. def get_years_until(target_amount):\\n2.\\n3.    current_fee = 623.00\\n4.    years = 0\\n5.    total_fee = current_fee\\n6.\\n7.    while total_fee < target_amount:\\n8.        total_fee = current_fee*(1.015**years)\\n9.        years += 1\\n10.\\n11.    return years\\n</bug_code>\\n<bug_desc>\\nOn line 8, the new total fee is calculated before the number of years is incremented, so the final result will be 1 greater than the correct answer.\\n</bug_desc>\\n<bug_fixes>\\nMove line 9 to be above line 8 and within the while loop.\\n</bug_fixes>\\n    </METADATA>\\n\\n    <CONVERSATION>\\n    User: I\\'m really stuck!\\nAssistant: </s>'"},"metadata":{}}]},{"cell_type":"code","source":"# EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n# # Modify formatting function to support batched=True\n# def formatting_prompts_func(batch):\n#     input_dialouges_batch = batch['input_dialouges']\n#     problem_metadata_batch = batch['problem_metadata']\n    \n#     texts = []\n#     for idx, input_dialouge in enumerate(input_dialouges_batch):\n#         text = prompt_style.format(problem_metadata_batch[idx], input_dialouges_batch[idx]) + EOS_TOKEN\n#         texts.append(text)\n#         # Optional: You can print or log the index if needed\n# #         print(f\"Processed item {idx}\")  # Remove this line in production if not needed\n#     return {\"text\": texts}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Prepare data in a format compatible with Dataset\n# data = {\n#     \"input_dialouges\": input_dialouges_dict,\n#     \"problem_metadata\": problem_metadata_dict,\n# }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Create Dataset from data\n# dataset = Dataset.from_dict(data)\n\n# # Apply formatting function with batched=True\n# dataset = dataset.map(\n#     formatting_prompts_func,\n#     batched=True\n# )\n\n# # Verify the first entry\n# print(dataset[\"text\"][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def construct_data(split_path: str):\n#     '''\n#     create a list of dictionaries for SFT\n#     '''\n    \n    \n\n# #     if preference_data:\n# #         # preference data\n# #         with open(os.path.join(data_path, 'preference_data.json'), 'r') as infile:\n# #             preference_data_dict = json.load(infile)\n    \n\n#     all_data = []\n\n#     for tr_file, metadata in tqdm(problem_metadata_dict.items(), total=len(problem_metadata_dict)):\n#         input_dialouges = input_dialouges_dict[tr_file]\n#         good_outputs_list = good_outputs_dict[tr_file]\n#         for ctr, dialouge in enumerate(input_dialouges):\n#             # construct prompt\n#             fix_prompt = construct_prompt(metadata, dialouge)\n# #             if not preference_data:\n                \n# #                 if split_path == 'testset':\n# #                     all_data.append({'prompt': fix_prompt, 'output': str(good_outputs_list[ctr])})\n# #                 else:\n#             for good_output in good_outputs_list[ctr]:\n#                 # append to all data\n#                 all_data.append({'prompt': fix_prompt, 'output': good_output+'</CONVERSATION>'})\n# #             else:\n# #                 preference_data_list = preference_data_dict[tr_file]\n# #                 # create dataset for DPO training\n# #                 for preference_tuples in preference_data_list[ctr]:\n# #                     # append to all data\n# #                     all_data.append({'prompt': fix_prompt, 'chosen': str(preference_tuples[0])+'</CONVERSATION>', 'rejected': str(preference_tuples[1])+'</CONVERSATION>'})\n                \n    \n#     return all_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def construct_data(split_path: str, preference_data=False):\n#     '''\n#     create a list of dictionaries for SFT\n#     '''\n    \n    \n#     data_path = os.path.join('/kaggle/input/', split_path)\n#     # input prompts \n#     with open(os.path.join(data_path, 'input_prompts.json'), 'r') as infile:\n#         input_dialouges_dict = json.load(infile)\n#     # problem metadata\n#     with open(os.path.join(data_path, 'problem_metadata.json'), 'r') as infile:\n#         problem_metadata_dict = json.load(infile)\n#     # good data \n#     with open(os.path.join(data_path, 'good_outputs.json'), 'r') as infile:\n#         good_outputs_dict = json.load(infile)\n#     if preference_data:\n#         # preference data\n#         with open(os.path.join(data_path, 'preference_data.json'), 'r') as infile:\n#             preference_data_dict = json.load(infile)\n    \n\n#     all_data = []\n\n#     for tr_file, metadata in tqdm(problem_metadata_dict.items(), total=len(problem_metadata_dict)):\n#         input_dialouges = input_dialouges_dict[tr_file]\n#         good_outputs_list = good_outputs_dict[tr_file]\n#         for ctr, dialouge in enumerate(input_dialouges):\n#             # construct prompt\n#             fix_prompt = construct_prompt(metadata, dialouge)\n#             if not preference_data:\n                \n#                 if split_path == 'testset':\n#                     all_data.append({'prompt': fix_prompt, 'output': str(good_outputs_list[ctr])})\n#                 else:\n#                     for good_output in good_outputs_list[ctr]:\n#                         # append to all data\n#                         all_data.append({'prompt': fix_prompt, 'output': good_output+'</CONVERSATION>'})\n#             else:\n#                 preference_data_list = preference_data_dict[tr_file]\n#                 # create dataset for DPO training\n#                 for preference_tuples in preference_data_list[ctr]:\n#                     # append to all data\n#                     all_data.append({'prompt': fix_prompt, 'chosen': str(preference_tuples[0])+'</CONVERSATION>', 'rejected': str(preference_tuples[1])+'</CONVERSATION>'})\n                \n    \n#     return all_data","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:02:06.247247Z","iopub.execute_input":"2024-10-31T11:02:06.247979Z","iopub.status.idle":"2024-10-31T11:02:06.261260Z","shell.execute_reply.started":"2024-10-31T11:02:06.247937Z","shell.execute_reply":"2024-10-31T11:02:06.260393Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# class QGSFTDataset(Dataset):\n#     '''\n#     QG Dataset\n#     '''\n#     def __init__(self, data: list):\n#         self.data = data\n#         self.column_names = list(data[0].keys())\n    \n#     def __getitem__(self, index: int):\n#         return self.data[index]\n\n#     def __len__(self):\n#         return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:11:05.947684Z","iopub.execute_input":"2024-10-31T11:11:05.948459Z","iopub.status.idle":"2024-10-31T11:11:05.955073Z","shell.execute_reply.started":"2024-10-31T11:11:05.948415Z","shell.execute_reply":"2024-10-31T11:11:05.954098Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# class QGSFTCollator:\n#     def __init__(self, tokenizer, test: bool):\n#         self.tokenizer = tokenizer\n#         self.test = test\n\n#     def __call__(self, batch):\n#         all_prompts = [sample[\"prompt\"] for sample in batch]\n#         prompts_tokenized = self.tokenizer(all_prompts, return_tensors=\"pt\", padding=True)\n#         if self.test:\n#             return {\n#                 \"input_ids\": prompts_tokenized.input_ids.to(device),\n#                 \"attention_mask\": prompts_tokenized.attention_mask.to(device),\n#                 \"meta_data\": batch\n#             }\n\n#         # TODO: might be worth debugging this\n#         all_inputs = [sample[\"prompt\"] + sample[\"output\"] + self.tokenizer.eos_token for sample in batch]\n#         inputs_tokenized = self.tokenizer(all_inputs, return_tensors=\"pt\", padding=True)\n#         prompt_lens = prompts_tokenized.attention_mask.sum(dim=1)\n#         labels = inputs_tokenized.input_ids.clone()\n#         padding_mask = torch.arange(labels.shape[1]).repeat(labels.shape[0], 1) < prompt_lens.unsqueeze(1)\n#         labels[padding_mask] = -100\n#         labels = labels.masked_fill(inputs_tokenized.attention_mask == 0, -100)\n#         return {\n#             \"input_ids\": inputs_tokenized.input_ids,\n#             \"attention_mask\": inputs_tokenized.attention_mask,\n#             \"labels\": labels\n#         }","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:02:16.506847Z","iopub.execute_input":"2024-10-31T11:02:16.507736Z","iopub.status.idle":"2024-10-31T11:02:16.517381Z","shell.execute_reply.started":"2024-10-31T11:02:16.507696Z","shell.execute_reply":"2024-10-31T11:02:16.516529Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# class QGDPODataset(Dataset):\n#     '''\n#     QG Dataset\n#     '''\n#     def __init__(self, data: list):\n#         self.data = data\n    \n#     def __getitem__(self, index: int):\n#         return self.data[index]\n\n#     def __len__(self):\n#         return len(self.data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TRAINING","metadata":{}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r=16, \n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n    lora_alpha=16,\n    lora_dropout=0, \n    bias=\"none\", \n   \n    use_gradient_checkpointing=\"unsloth\", \n    random_state=3407,\n    use_rslora=False, \n    loftq_config=None,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:06:30.058751Z","iopub.execute_input":"2024-10-31T13:06:30.059456Z","iopub.status.idle":"2024-10-31T13:06:38.111593Z","shell.execute_reply.started":"2024-10-31T13:06:30.059414Z","shell.execute_reply":"2024-10-31T13:06:38.110597Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"Unsloth 2024.10.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}]},{"cell_type":"code","source":"# def get_training_args(model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb):\n#     return TrainingArguments(\n#         output_dir=model_name,\n#         num_train_epochs=epochs,\n#         learning_rate=lr,\n#         weight_decay=wd,\n#         max_grad_norm=max_grad_norm or None,\n#         per_device_train_batch_size=batch_size,\n#         gradient_accumulation_steps=grad_accum_steps,\n#         per_device_eval_batch_size=batch_size * 2,\n#         eval_accumulation_steps=4,\n#         warmup_ratio=0.1,\n#         eval_strategy=\"epoch\",\n#         save_strategy=\"epoch\",\n#         save_total_limit=1,\n#         load_best_model_at_end=True,\n#         metric_for_best_model=\"loss\",\n#         greater_is_better=False,\n#         remove_unused_columns=False,\n#         logging_steps=1,               # Log training loss every batch\n#         log_level='info',\n#         report_to=\"wandb\" if wandb else \"none\"\n#     )","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:02:32.931873Z","iopub.execute_input":"2024-10-31T11:02:32.932756Z","iopub.status.idle":"2024-10-31T11:02:32.939908Z","shell.execute_reply.started":"2024-10-31T11:02:32.932715Z","shell.execute_reply":"2024-10-31T11:02:32.938882Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# all_train_data = construct_data(split_path='train-data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_train_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data, val_data = train_test_split(all_train_data, test_size=0.2, random_state=37)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_set = QGSFTDataset(train_data)\n# train_set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:17:37.052180Z","iopub.execute_input":"2024-10-31T13:17:37.052590Z","iopub.status.idle":"2024-10-31T13:17:38.760359Z","shell.execute_reply.started":"2024-10-31T13:17:37.052525Z","shell.execute_reply":"2024-10-31T13:17:38.759363Z"},"trusted":true},"execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/763 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1da6ab515e7a42c88409172ac993659f"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:19:12.022859Z","iopub.execute_input":"2024-10-31T13:19:12.023261Z","iopub.status.idle":"2024-10-31T14:26:05.139867Z","shell.execute_reply.started":"2024-10-31T13:19:12.023225Z","shell.execute_reply":"2024-10-31T14:26:05.139009Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers, TRL and Unsloth!\n`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n","output_type":"stream"},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 763 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 60\n \"-____-\"     Number of trainable parameters = 41,943,040\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 1:05:20, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.191300</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.357200</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.237900</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.150800</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.011100</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.972100</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.846500</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.697100</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.564100</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.595700</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.681300</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.565800</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.538200</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.662300</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.634900</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.655000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.578300</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.506000</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.450300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.505100</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.494000</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.410100</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.493900</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.454000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.471600</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.508500</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.410900</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.368500</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.323000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.444900</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.395700</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.346200</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.423900</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.399400</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.458700</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.388400</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.395000</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.352100</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.308000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.323100</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.374600</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.388900</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.246700</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.265400</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.364500</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.282800</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.308700</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.270700</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.196600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.272600</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.251500</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.383300</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.191700</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.240100</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.248800</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.217700</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.200700</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.256700</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.319000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.267200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"new_model_online = \"vihidun/unsloth_mistral-7b-bnb-4bit\"\nnew_model_local = \"mistral-7b-bnb-4bit\"\nmodel.save_pretrained(new_model_local) # Local saving\ntokenizer.save_pretrained(new_model_local) # Local saving","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:04:52.927206Z","iopub.execute_input":"2024-10-31T15:04:52.927609Z","iopub.status.idle":"2024-10-31T15:04:53.656600Z","shell.execute_reply.started":"2024-10-31T15:04:52.927568Z","shell.execute_reply":"2024-10-31T15:04:53.655564Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"('mistral-7b-bnb-4bit/tokenizer_config.json',\n 'mistral-7b-bnb-4bit/special_tokens_map.json',\n 'mistral-7b-bnb-4bit/tokenizer.model',\n 'mistral-7b-bnb-4bit/added_tokens.json',\n 'mistral-7b-bnb-4bit/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(new_model_online) # Online saving\ntokenizer.push_to_hub(new_model_online) # Online saving","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:04:55.394896Z","iopub.execute_input":"2024-10-31T15:04:55.395657Z","iopub.status.idle":"2024-10-31T15:05:06.638484Z","shell.execute_reply.started":"2024-10-31T15:04:55.395617Z","shell.execute_reply":"2024-10-31T15:05:06.637582Z"},"trusted":true},"execution_count":78,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a220de27ed9f414d9ff5e7c73ca62540"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/vihidun/unsloth_mistral-7b-bnb-4bit\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"960103368cbf4c2daee2edd0f9ea5ce7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03d037d84d9040cab27ba9712707d957"}},"metadata":{}}]},{"cell_type":"code","source":"max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:06:07.176177Z","iopub.execute_input":"2024-10-31T15:06:07.176594Z","iopub.status.idle":"2024-10-31T15:06:07.182389Z","shell.execute_reply.started":"2024-10-31T15:06:07.176555Z","shell.execute_reply":"2024-10-31T15:06:07.181455Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"Vihidun/unsloth_mistral-7b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:06:31.876458Z","iopub.execute_input":"2024-10-31T15:06:31.876855Z","iopub.status.idle":"2024-10-31T15:06:50.667475Z","shell.execute_reply.started":"2024-10-31T15:06:31.876819Z","shell.execute_reply":"2024-10-31T15:06:50.666445Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2024.10.7: Fast Mistral patching. Transformers = 4.44.2.\n   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.888 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.5.1+cu121. CUDA = 6.0. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3d0a45286ab4c82add9f12aa256a871"}},"metadata":{}}]},{"cell_type":"code","source":"# alpaca_prompt = Copied from above\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    prompt_style.format(\n        \"<problem>\\nWrite a function `factorial(n:int) -> int` that computes the factorial n! of a natural number n, which is defined mathematically as:\\n\\n$0! = 1$\\n$n! = n \\\\times (n - 1)!$\\n\\nAdditionally, if the input integer n is negative the function should return 0.\\n\\n## Example Cases:\\n```\\nfactorial(-1) => 0\\nfactorial(0) => 1\\nfactorial(1) => 1\\nfactorial(2) => 2\\nfactorial(3) => 6\\nfactorial(4) => 24\\nfactorial(5) => 120\\n```\\n</problem>\\n<bug_code>\\n1. def factorial(n):\\n2.        if n < 0:\\n3.                return 0\\n4.        fact = 1\\n5.        for i in range(n):\\n6.                fact = fact * i\\n7.        return fact\\n</bug_code>\\n<bug_desc>\\nOn line 6, `fact` is multiplied with 0 in the first iteration of the for loop. Consequently, at every iteration fact stays equal with 0 instead of being updated to be equal with factorial of `(i + 1)`. Therefore, the function will return 0, irrespective of n\\n</bug_desc>\\n<bug_fixes>\\nReplace `i` with `(i + 1)` in line 6.\\nReplace `range(n)` with `range(1, n + 1)` in line 5.\\n</bug_fixes>\", \n        \"User: Hi! I implemented the factorial function but it doesn\\u2019t work and I do not know why. Can you help?\\nAssistant: \", \n        \"\"\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\nmodel_output = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:10:02.975030Z","iopub.execute_input":"2024-10-31T15:10:02.975744Z","iopub.status.idle":"2024-10-31T15:10:04.505995Z","shell.execute_reply.started":"2024-10-31T15:10:02.975702Z","shell.execute_reply":"2024-10-31T15:10:04.505188Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"<s> \n    [INST] Generate \"socratic\" guidance as a \"Socratic Guiding Assistant\" to help the User debug their code. The generated guidance must help the User realize their bug. The guidance must not directly reveal the bug fix, and must be coherent with the conversation so far.\n    You are given the following metadata:\n    1. Problem Description and Test Cases (<problem>)\n    2. Student's buggy code (<bug_code>)\n    3. Bug Description (<bug_desc>)\n    4. Bug Fixes (<bug_fixes>)\n    5. Conversation (between User and Assistant) so far (<CONVERSATION>)[/INST]\n\n    <METADATA>\n    <problem>\nWrite a function `factorial(n:int) -> int` that computes the factorial n! of a natural number n, which is defined mathematically as:\n\n$0! = 1$\n$n! = n \\times (n - 1)!$\n\nAdditionally, if the input integer n is negative the function should return 0.\n\n## Example Cases:\n```\nfactorial(-1) => 0\nfactorial(0) => 1\nfactorial(1) => 1\nfactorial(2) => 2\nfactorial(3) => 6\nfactorial(4) => 24\nfactorial(5) => 120\n```\n</problem>\n<bug_code>\n1. def factorial(n):\n2.        if n < 0:\n3.                return 0\n4.        fact = 1\n5.        for i in range(n):\n6.                fact = fact * i\n7.        return fact\n</bug_code>\n<bug_desc>\nOn line 6, `fact` is multiplied with 0 in the first iteration of the for loop. Consequently, at every iteration fact stays equal with 0 instead of being updated to be equal with factorial of `(i + 1)`. Therefore, the function will return 0, irrespective of n\n</bug_desc>\n<bug_fixes>\nReplace `i` with `(i + 1)` in line 6.\nReplace `range(n)` with `range(1, n + 1)` in line 5.\n</bug_fixes>\n    </METADATA>\n\n    <CONVERSATION>\n    User: Hi! I implemented the factorial function but it doesnâ€™t work and I do not know why. Can you help?\nAssistant: </s>\n","output_type":"stream"}]},{"cell_type":"code","source":"# from trl import SFTTrainer\n# from transformers import TrainingArguments\n# from unsloth import is_bfloat16_supported\n\n# def sft(base_model, model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb):\n    \n#     # construct data\n#     print('#### Constructing Data ####')\n#     all_train_data = construct_data(split_path='train-data')\n    \n#     print(\"#### Splitting Data ####\")\n#     # split into train and val (80-20)\n#     train_data, val_data = train_test_split(all_train_data, test_size=0.2, random_state=37)\n    \n#     trainer = SFTTrainer(\n#         model = model,\n\n#         train_dataset=QGSFTDataset(train_data),\n#         eval_dataset=QGSFTDataset(val_data),\n#         data_collator=QGSFTCollator(tokenizer, False),\n        \n# #         dataset_text_field = \"text\",\n# #         max_seq_length = max_seq_length,\n# #         dataset_num_proc = 2,\n#         args=get_training_args(model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb),\n\n#     )\n    \n#     print(\"#### train model ####\")\n#     trainer.train()\n    \n#     print(\"#### save model ####\")\n#     model_save_path = f\"/kaggle/working/{model_name}_final_model\"\n#     trainer.save_model(model_save_path)\n    \n#     print(\"#### Log model to WandB ####\")\n#     # Create and log a WandB artifact for the trained model\n#     artifact = wandb.Artifact(name=model_name, type=\"model\")\n#     artifact.add_dir(model_save_path)  # Add the model directory to the artifact\n#     wandb.log_artifact(artifact)  # Log the artifact to WandB\n    \n#     print(\"Model saved and logged to WandB successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:02:56.137037Z","iopub.execute_input":"2024-10-31T11:02:56.138072Z","iopub.status.idle":"2024-10-31T11:02:56.149796Z","shell.execute_reply.started":"2024-10-31T11:02:56.138019Z","shell.execute_reply":"2024-10-31T11:02:56.148930Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# def sft(base_model, model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb):\n# #     assert args.model_name\n\n\n#     # construct data\n#     print('#### Constructing Data ####')\n#     all_train_data = construct_data(split_path='train-data')\n    \n#     print(\"#### Splitting Data ####\")\n#     # split into train and val (80-20)\n#     train_data, val_data = train_test_split(all_train_data, test_size=0.2, random_state=37)\n    \n#     print('#### get model and tokenizer ####')\n#     model, tokenizer = get_model(base_model, None, None, False, False)\n    \n#     print(\"#### Run training configuration ####\")\n#     trainer = Trainer(\n#         model=model,\n#         args=get_training_args(model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb),\n#         train_dataset=QGSFTDataset(train_data),\n#         eval_dataset=QGSFTDataset(val_data),\n#         data_collator=QGSFTCollator(tokenizer, False)\n#     )\n    \n#     print(\"#### train model ####\")\n#     trainer.train()\n    \n#     print(\"#### save model ####\")\n#     trainer.save_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SANITY CHECKS","metadata":{}},{"cell_type":"code","source":"# def check_max_token_len(base_model, split_type='train'):\n#     # load data \n#     if split_type == 'train':\n#         data = construct_data(split_path='train-data')\n#     elif split_type == 'testset':\n#         data = construct_data(split_path='test-data')\n\n#     tokenizer = AutoTokenizer.from_pretrained(base_model)\n#     tokenizer.pad_token = tokenizer.eos_token\n#     tokenizer.padding_side = \"right\"\n#     max_len_full_prompt = 0\n#     max_len_full_output = 0\n#     for sample in tqdm(data, total = len(data)):\n#         input_sample = sample['prompt'] + sample['output'] + tokenizer.eos_token\n#         tokenized = tokenizer.encode(input_sample)\n#         tokenize_output = tokenizer.encode(sample['output'])\n#         max_len_full_prompt = max(max_len_full_prompt, len(tokenized))\n#         max_len_full_output = max(max_len_full_output, len(tokenize_output))\n    \n#     print('#### Split Type: {} ####'.format(split_type))\n#     print(f\"Maximum tokenized length: {max_len_full_prompt}\")\n#     print(f\"Maximum tokenized length: {max_len_full_output}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def perform_sanity_checks(base_model):\n#     # check maximum tokenized length of the inputs \n#     check_max_token_len(base_model, split_type='train')\n#     check_max_token_len(base_model, split_type='testset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MAIN","metadata":{}},{"cell_type":"code","source":"# python finetune/sft_dpo.py --sft --base_model codellama/CodeLlama-7b-Instruct-hf --model_name codellama_sft_b2 --batch_size 2 --grad_accum_steps 32 --epochs 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # base_model = \"codellama/CodeLlama-7b-Instruct-hf\"\n# # base_model = \"microsoft/Phi-3.5-mini-instruct\"\n# base_model = \"unsloth/mistral-7b-bnb-4bit\"\n# # model_name = \"codellama_sft_b2\"\n# model_name = \"mistral-7b-bnb-4bit_sft_b2\"\n# batch_size = 2\n# grad_accum_steps = 32\n# epochs = 5\n# max_grad_norm = 1.0\n# lr = 3e-5\n# wd = 0.0\n# wandb = \"wandb\"","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:18:50.002174Z","iopub.execute_input":"2024-10-31T13:18:50.003154Z","iopub.status.idle":"2024-10-31T13:18:50.008433Z","shell.execute_reply.started":"2024-10-31T13:18:50.003109Z","shell.execute_reply":"2024-10-31T13:18:50.007707Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# torch.cuda.empty_cache()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sft(base_model, model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:18:53.262383Z","iopub.execute_input":"2024-10-31T13:18:53.263029Z","iopub.status.idle":"2024-10-31T13:18:53.343460Z","shell.execute_reply.started":"2024-10-31T13:18:53.262990Z","shell.execute_reply":"2024-10-31T13:18:53.342298Z"},"trusted":true},"execution_count":58,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msft\u001b[49m(base_model, model_name, epochs, lr, wd, max_grad_norm, batch_size, grad_accum_steps, wandb)\n","\u001b[0;31mNameError\u001b[0m: name 'sft' is not defined"],"ename":"NameError","evalue":"name 'sft' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import shutil\n\n# Create a zip file of the directory\nshutil.make_archive('/kaggle/working/wandb', 'zip', '/kaggle/working/wandb')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T14:48:31.298973Z","iopub.execute_input":"2024-10-31T14:48:31.299681Z","iopub.status.idle":"2024-10-31T14:48:31.332363Z","shell.execute_reply.started":"2024-10-31T14:48:31.299641Z","shell.execute_reply":"2024-10-31T14:48:31.331512Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/wandb.zip'"},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Display a download link\nFileLink(r'/kaggle/working/outputs.zip')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T14:44:18.179549Z","iopub.execute_input":"2024-10-31T14:44:18.180406Z","iopub.status.idle":"2024-10-31T14:44:18.188016Z","shell.execute_reply.started":"2024-10-31T14:44:18.180361Z","shell.execute_reply":"2024-10-31T14:44:18.187120Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/outputs.zip","text/html":"<a href='/kaggle/working/outputs.zip' target='_blank'>/kaggle/working/outputs.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import display, Markdown\n\nFastLanguageModel.for_inference(model)\ninputs = tokenizer(\n    [\n        prompt_style.format(\n            \"<problem>\\nWrite a function `factorial(n:int) -> int` that computes the factorial n! of a natural number n, which is defined mathematically as:\\n\\n$0! = 1$\\n$n! = n \\\\times (n - 1)!$\\n\\nAdditionally, if the input integer n is negative the function should return 0.\\n\\n## Example Cases:\\n```\\nfactorial(-1) => 0\\nfactorial(0) => 1\\nfactorial(1) => 1\\nfactorial(2) => 2\\nfactorial(3) => 6\\nfactorial(4) => 24\\nfactorial(5) => 120\\n```\\n</problem>\\n<bug_code>\\n1. def factorial(n):\\n2.        if n < 0:\\n3.                return 0\\n4.        fact = 1\\n5.        for i in range(n):\\n6.                fact = fact * i\\n7.        return fact\\n</bug_code>\\n<bug_desc>\\nOn line 6, `fact` is multiplied with 0 in the first iteration of the for loop. Consequently, at every iteration fact stays equal with 0 instead of being updated to be equal with factorial of `(i + 1)`. Therefore, the function will return 0, irrespective of n\\n</bug_desc>\\n<bug_fixes>\\nReplace `i` with `(i + 1)` in line 6.\\nReplace `range(n)` with `range(1, n + 1)` in line 5.\\n</bug_fixes>\", \n            \"User: Hi! I implemented the factorial function but it doesn\\u2019t work and I do not know why. Can you help?\\nAssistant: \", \n        )\n    ],\n    return_tensors=\"pt\",\n).to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=512,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\n# Markdown(response[0].split(\"\\n\\n### Response:\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-10-31T14:57:59.317728Z","iopub.execute_input":"2024-10-31T14:57:59.318131Z","iopub.status.idle":"2024-10-31T14:58:00.817902Z","shell.execute_reply.started":"2024-10-31T14:57:59.318096Z","shell.execute_reply":"2024-10-31T14:58:00.816961Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"prompt = prompt_style.format(\n            \"<problem>\\nWrite a function `factorial(n:int) -> int` that computes the factorial n! of a natural number n, which is defined mathematically as:\\n\\n$0! = 1$\\n$n! = n \\\\times (n - 1)!$\\n\\nAdditionally, if the input integer n is negative the function should return 0.\\n\\n## Example Cases:\\n```\\nfactorial(-1) => 0\\nfactorial(0) => 1\\nfactorial(1) => 1\\nfactorial(2) => 2\\nfactorial(3) => 6\\nfactorial(4) => 24\\nfactorial(5) => 120\\n```\\n</problem>\\n<bug_code>\\n1. def factorial(n):\\n2.        if n < 0:\\n3.                return 0\\n4.        fact = 1\\n5.        for i in range(n):\\n6.                fact = fact * i\\n7.        return fact\\n</bug_code>\\n<bug_desc>\\nOn line 6, `fact` is multiplied with 0 in the first iteration of the for loop. Consequently, at every iteration fact stays equal with 0 instead of being updated to be equal with factorial of `(i + 1)`. Therefore, the function will return 0, irrespective of n\\n</bug_desc>\\n<bug_fixes>\\nReplace `i` with `(i + 1)` in line 6.\\nReplace `range(n)` with `range(1, n + 1)` in line 5.\\n</bug_fixes>\", \n            \"User: Hi! I implemented the factorial function but it doesn\\u2019t work and I do not know why. Can you help?\\nAssistant: \", \n        )\nprint(prompt)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T14:57:40.659440Z","iopub.execute_input":"2024-10-31T14:57:40.659834Z","iopub.status.idle":"2024-10-31T14:57:40.667156Z","shell.execute_reply.started":"2024-10-31T14:57:40.659796Z","shell.execute_reply":"2024-10-31T14:57:40.666162Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"\n    [INST] Generate \"socratic\" guidance as a \"Socratic Guiding Assistant\" to help the User debug their code. The generated guidance must help the User realize their bug. The guidance must not directly reveal the bug fix, and must be coherent with the conversation so far.\n    You are given the following metadata:\n    1. Problem Description and Test Cases (<problem>)\n    2. Student's buggy code (<bug_code>)\n    3. Bug Description (<bug_desc>)\n    4. Bug Fixes (<bug_fixes>)\n    5. Conversation (between User and Assistant) so far (<CONVERSATION>)[/INST]\n\n    <METADATA>\n    <problem>\nWrite a function `factorial(n:int) -> int` that computes the factorial n! of a natural number n, which is defined mathematically as:\n\n$0! = 1$\n$n! = n \\times (n - 1)!$\n\nAdditionally, if the input integer n is negative the function should return 0.\n\n## Example Cases:\n```\nfactorial(-1) => 0\nfactorial(0) => 1\nfactorial(1) => 1\nfactorial(2) => 2\nfactorial(3) => 6\nfactorial(4) => 24\nfactorial(5) => 120\n```\n</problem>\n<bug_code>\n1. def factorial(n):\n2.        if n < 0:\n3.                return 0\n4.        fact = 1\n5.        for i in range(n):\n6.                fact = fact * i\n7.        return fact\n</bug_code>\n<bug_desc>\nOn line 6, `fact` is multiplied with 0 in the first iteration of the for loop. Consequently, at every iteration fact stays equal with 0 instead of being updated to be equal with factorial of `(i + 1)`. Therefore, the function will return 0, irrespective of n\n</bug_desc>\n<bug_fixes>\nReplace `i` with `(i + 1)` in line 6.\nReplace `range(n)` with `range(1, n + 1)` in line 5.\n</bug_fixes>\n    </METADATA>\n\n    <CONVERSATION>\n    User: Hi! I implemented the factorial function but it doesnâ€™t work and I do not know why. Can you help?\nAssistant: \n","output_type":"stream"}]},{"cell_type":"code","source":"inputs","metadata":{"execution":{"iopub.status.busy":"2024-10-31T14:57:11.947393Z","iopub.execute_input":"2024-10-31T14:57:11.947796Z","iopub.status.idle":"2024-10-31T14:57:11.977993Z","shell.execute_reply.started":"2024-10-31T14:57:11.947757Z","shell.execute_reply":"2024-10-31T14:57:11.976970Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[    1, 28705,    13,  2287,   733, 16289, 28793, 26075,   345, 28713,\n         26184, 28739, 15988,   390,   264,   345, 28735, 26184,  2480,  3269,\n         21631, 28739,   298,  1316,   272,  1247, 10183,   652,  2696, 28723,\n           415,  7138, 15988,  1580,  1316,   272,  1247,  8168,   652, 10079,\n         28723,   415, 15988,  1580,   459,  5090, 14918,   272, 10079,  6293,\n         28725,   304,  1580,   347,  1001, 21891,   395,   272,  7114,   579,\n          2082, 28723,    13,  2287,   995,   460,  2078,   272,  2296, 11214,\n         28747,    13,   260, 28740, 28723, 24857, 10220,   304,  3735,   334,\n          2018,   325, 28789,   729, 11706, 12970,    13,   260, 28750, 28723,\n         15965, 28742, 28713, 10079,  1495,  2696,   325, 28789,  3718, 28730,\n          1409, 12970,    13,   260, 28770, 28723, 27431, 10220,   325, 28789,\n          3718, 28730,  5916, 12970,    13,   260, 28781, 28723, 27431, 23047,\n           274,   325, 28789,  3718, 28730,  7192,   274, 12970,    13,   260,\n         28782, 28723,  1325, 25422,   325, 24829,  1247,   304, 21631, 28731,\n           579,  2082,   325, 28789,  3185,  3491, 28735,  4866, 28767, 10908,\n         28748, 16289, 28793,    13,    13,  2287,   523,  1574, 28738,  1841,\n          5854, 28767,    13,  2287,   523,   729, 11706, 28767,    13,  5238,\n           264,   908,  1552, 18360,   505, 28732, 28711, 28747,   501, 28731,\n          3193,   716, 28832,   369,   623,  2907,   272,  6999,   505,   307,\n         28808,   302,   264,  4229,  1474,   307, 28725,   690,   349,  4057,\n          1610,  6981,  6789,   390, 28747,    13,    13, 28776, 28734, 28808,\n           327, 28705, 28740, 28776,    13, 28776, 28711, 28808,   327,   307,\n           414,  4593,   325, 28711,   387, 28705, 28740, 28731, 28808, 28776,\n            13,    13,  1991, 14586, 28725,   513,   272,  2787, 11584,   307,\n           349,  7087,   272,   908,  1023,   604, 28705, 28734, 28723,    13,\n            13,  1064, 16693,   334,  2018, 28747,    13, 13940, 28832,    13,\n         18360,   505,  6422, 28740, 28731,   953, 28705, 28734,    13, 18360,\n           505, 28732, 28734, 28731,   953, 28705, 28740,    13, 18360,   505,\n         28732, 28740, 28731,   953, 28705, 28740,    13, 18360,   505, 28732,\n         28750, 28731,   953, 28705, 28750,    13, 18360,   505, 28732, 28770,\n         28731,   953, 28705, 28784,    13, 18360,   505, 28732, 28781, 28731,\n           953, 28705, 28750, 28781,    13, 18360,   505, 28732, 28782, 28731,\n           953, 28705, 28740, 28750, 28734,    13, 13940, 28832,    13,   700,\n           729, 11706, 28767,    13, 28789,  3718, 28730,  1409, 28767,    13,\n         28740, 28723,   801,  6999,   505, 28732, 28711,  1329,    13, 28750,\n         28723,  5390,   513,   307,   523, 28705, 28734, 28747,    13, 28770,\n         28723,  1417, 28705,   604, 28705, 28734,    13, 28781, 28723,  5390,\n          1639,   327, 28705, 28740,    13, 28782, 28723,  5390,   354,   613,\n           297,  2819, 28732, 28711,  1329,    13, 28784, 28723,  1417, 28705,\n          1639,   327,  1639,   398,   613,    13, 28787, 28723,  5390,   604,\n          1639,    13,   700,  3718, 28730,  1409, 28767,    13, 28789,  3718,\n         28730,  5916, 28767,    13,  2486,  1407, 28705, 28784, 28725,  1552,\n         22313, 28832,   349,  6079,  3002,   395, 28705, 28734,   297,   272,\n           907, 23113,   302,   272,   354,  7870, 28723,  1325, 23321, 28725,\n           438,  1012, 23113,  1639, 22361,  6530,   395, 28705, 28734,  3519,\n           302,  1250,  8457,   298,   347,  6530,   395,  6999,   505,   302,\n          1552, 28732, 28710,   648, 28705, 28740, 28731,  9429,  8469, 28725,\n           272,   908,   622,   604, 28705, 28734, 28725,  4139,   411,  8524,\n           302,   307,    13,   700,  3718, 28730,  5916, 28767,    13, 28789,\n          3718, 28730,  7192,   274, 28767,    13, 21392,  1552, 28710, 28832,\n           395,  1552, 28732, 28710,   648, 28705, 28740, 28731, 28832,   297,\n          1407, 28705, 28784, 28723,    13, 21392,  1552,  6347, 28732, 28711,\n         28731, 28832,   395,  1552,  6347, 28732, 28740, 28725,   307,   648,\n         28705, 28740, 28731, 28832,   297,  1407, 28705, 28782, 28723,    13,\n           700,  3718, 28730,  7192,   274, 28767,    13,  2287,  1867,  1574,\n         28738,  1841,  5854, 28767,    13,    13,  2287,   523,  3185,  3491,\n         28735,  4866, 28767,    13,  2287,  1247, 28747, 15359, 28808,   315,\n         13492,   272,  6999,   505,   908,   562,   378,  2368, 28809, 28707,\n           771,   304,   315,   511,   459,   873,  2079, 28723,  2418,   368,\n          1316, 28804,    13,  7226, 11143, 28747, 28705]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n       device='cuda:0')}"},"metadata":{}}]},{"cell_type":"code","source":"response","metadata":{"execution":{"iopub.status.busy":"2024-10-31T14:58:06.957555Z","iopub.execute_input":"2024-10-31T14:58:06.957941Z","iopub.status.idle":"2024-10-31T14:58:06.965962Z","shell.execute_reply.started":"2024-10-31T14:58:06.957905Z","shell.execute_reply":"2024-10-31T14:58:06.965111Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"['<s> \\n    [INST] Generate \"socratic\" guidance as a \"Socratic Guiding Assistant\" to help the User debug their code. The generated guidance must help the User realize their bug. The guidance must not directly reveal the bug fix, and must be coherent with the conversation so far.\\n    You are given the following metadata:\\n    1. Problem Description and Test Cases (<problem>)\\n    2. Student\\'s buggy code (<bug_code>)\\n    3. Bug Description (<bug_desc>)\\n    4. Bug Fixes (<bug_fixes>)\\n    5. Conversation (between User and Assistant) so far (<CONVERSATION>)[/INST]\\n\\n    <METADATA>\\n    <problem>\\nWrite a function `factorial(n:int) -> int` that computes the factorial n! of a natural number n, which is defined mathematically as:\\n\\n$0! = 1$\\n$n! = n \\\\times (n - 1)!$\\n\\nAdditionally, if the input integer n is negative the function should return 0.\\n\\n## Example Cases:\\n```\\nfactorial(-1) => 0\\nfactorial(0) => 1\\nfactorial(1) => 1\\nfactorial(2) => 2\\nfactorial(3) => 6\\nfactorial(4) => 24\\nfactorial(5) => 120\\n```\\n</problem>\\n<bug_code>\\n1. def factorial(n):\\n2.        if n < 0:\\n3.                return 0\\n4.        fact = 1\\n5.        for i in range(n):\\n6.                fact = fact * i\\n7.        return fact\\n</bug_code>\\n<bug_desc>\\nOn line 6, `fact` is multiplied with 0 in the first iteration of the for loop. Consequently, at every iteration fact stays equal with 0 instead of being updated to be equal with factorial of `(i + 1)`. Therefore, the function will return 0, irrespective of n\\n</bug_desc>\\n<bug_fixes>\\nReplace `i` with `(i + 1)` in line 6.\\nReplace `range(n)` with `range(1, n + 1)` in line 5.\\n</bug_fixes>\\n    </METADATA>\\n\\n    <CONVERSATION>\\n    User: Hi! I implemented the factorial function but it doesnâ€™t work and I do not know why. Can you help?\\nAssistant: </s>']"},"metadata":{}}]}]}